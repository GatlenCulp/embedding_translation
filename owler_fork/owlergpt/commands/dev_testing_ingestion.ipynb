{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Testing me========================================\n",
      "WARNING: CUDA is not available, using CPU\n",
      "['BAAI/bge-small-en-v1.5', 'intfloat/e5-small-v2', 'thenlper/gte-small']\n",
      "[SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      "), SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      "), SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")]\n",
      "['bge-small-en-v1.5', 'e5-small-v2', 'gte-small']\n",
      "[<langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter object at 0x33ef54490>, <langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter object at 0x178539fd0>, <langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter object at 0x3c1c398d0>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is a simple jupyter notebook just for the purposes of testing the ingestion pipeline. It tests that following works as desired:\n",
    "- Ingestion functionality (using the test dataset)\n",
    "- Embedding functionality (using the test dataset, small so can be run on CPU with a small model)\n",
    "- Fixing metadata\n",
    "\n",
    "The entire process works like this\n",
    "(1) ingestion [dataset download, embedding, documentation: text2dataset] => (2) training [training layers: dataset2dataset] => (3) comparison/analsis [dataset2viz]\n",
    "\n",
    "Ingestion refers to:\n",
    "1. Downloading the dataset from huggingface (this is done via `download_datsets.py` in `flask download_ds`)\n",
    "2. Ingesting the dataset (this is done via `ingest_ds.py` in `flask ingest_ds` and also `../modern/ingestion.py`)\n",
    "3. Fixing the metadata (this is done via `fix_metadata.py` in `flask metadata_ds` + `../modern/ingestion.py`)\n",
    "    (there is help from `model_sizes.py` among others).\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from owlergpt.modern.collection_utils import MODEL_NAMES\n",
    "\n",
    "\n",
    "print(\"=\" * 40 + \" Testing me\" + \"=\" * 40)\n",
    "if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = click.prompt(\n",
    "        \"Please enter the CUDA_VISIBLE_DEVICES value\", type=str\n",
    "    )\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: CUDA is not available, using CPU\")\n",
    "else:\n",
    "    print(f\"Using CUDA device {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "\n",
    "# default parameters\n",
    "selected_folder = \"test_dataset\"\n",
    "tokens_per_chunk = 256\n",
    "chunk_overlap = 25\n",
    "\n",
    "# Filter for small models, cap at 3\n",
    "small_model_names = [m for m in MODEL_NAMES if \"small\" in m.lower()][:3]\n",
    "assert all(\"/\" in m for m in small_model_names), f\"Expected all small models to be HF models, got {small_model_names}\"  # fmt: skip\n",
    "assert (\n",
    "    len(small_model_names) == 3\n",
    "), f\"Expected 3 small models, got {len(small_model_names)}\"  # should be multiple\n",
    "if len(small_model_names) == 0:\n",
    "    raise ValueError(\"No small models found in OPENAI_MODELS\")\n",
    "models = [\n",
    "    SentenceTransformer(model_name, device=device) for model_name in small_model_names\n",
    "]\n",
    "model_names = [\n",
    "    s_model_name.split(\"/\")[-1] for s_model_name in small_model_names\n",
    "]  # get the model names for saving\n",
    "text_splitters = [\n",
    "    SentenceTransformersTokenTextSplitter(\n",
    "        model_name=s_model_name,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        tokens_per_chunk=tokens_per_chunk,\n",
    "    )\n",
    "    for s_model_name in small_model_names\n",
    "]\n",
    "print(small_model_names)\n",
    "print(models)\n",
    "print(model_names)\n",
    "print(text_splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumps over the lazy dog.', 'A journey of a thousand miles begins with a single step.', 'All that glitters is not gold.', 'Actions speak louder than words.', 'Beauty is in the eye of the beholder.', 'Every cloud has a silver lining.', 'Fortune favors the bold.', 'Knowledge is power.', 'Practice makes perfect.', 'Time heals all wounds.', 'donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, donkey is happy, ']\n",
      "['test_record_0', 'test_record_1', 'test_record_2', 'test_record_3', 'test_record_4', 'test_record_5', 'test_record_6', 'test_record_7', 'test_record_8', 'test_record_9', 'test_record_10']\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Beauty is in the eye of the beholder.\",\n",
    "    \"Every cloud has a silver lining.\",\n",
    "    \"Fortune favors the bold.\",\n",
    "    \"Knowledge is power.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Time heals all wounds.\",\n",
    "    # add one long entry here so that we can pass the 256 limit\n",
    "    \"donkey is happy, \" * 400,  # surely at least 2 chunks at least with toks\n",
    "]\n",
    "test_queries = [\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Do donkeys like apples?\",\n",
    "    \"Where is 4??\",\n",
    "]\n",
    "record_ids = [f\"test_record_{i}\" for i in range(len(test_sentences))]\n",
    "print(test_sentences)\n",
    "print(record_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset and regular collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating collections: 3it [00:01,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating collections\n",
      "[Collection(name=test_dataset_bge-small-en-v1.5_CharacterSplitting_256), Collection(name=test_dataset_e5-small-v2_CharacterSplitting_256), Collection(name=test_dataset_gte-small_CharacterSplitting_256)]\n",
      "Using JSONDataset to embed...\n",
      ">>>> >>>> >>>> >>>> 1. Creating JSONL files\n",
      ">>>> >>>> >>>> >>>> 2. Creating JSONL files using the default ingestion pipeline\n",
      ">>>> >>>> >>>> >>>> 3. Creating collection for BAAI/bge-small-en-v1.5 w/ test_dataset_0 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_0) (1/6)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset-specific DB test_dataset_0_256 to store embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 304.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_0\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 51.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 documents, generated 13 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 53.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 documents, generated 26 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> >>>> >>>> >>>> 3. Creating collection for BAAI/bge-small-en-v1.5 w/ test_dataset_1 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_1) (2/6)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset-specific DB test_dataset_1_256 to store embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 300.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_1\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 3/3 [00:00<00:00, 102.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 documents, generated 1 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 3/3 [00:00<00:00, 149.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 documents, generated 2 embeddings.\n",
      ">>>> >>>> >>>> >>>> 3. Creating collection for intfloat/e5-small-v2 w/ test_dataset_0 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_0) (3/6)\n",
      "Getting text_splitter, transformer_model, client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-specific DB test_dataset_0_256 already exists. Using it to store embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 239.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_0\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 52.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 documents, generated 13 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 59.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 documents, generated 26 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> >>>> >>>> >>>> 3. Creating collection for intfloat/e5-small-v2 w/ test_dataset_1 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_1) (4/6)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-specific DB test_dataset_1_256 already exists. Using it to store embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 426.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_1\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 3/3 [00:00<00:00, 85.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 documents, generated 1 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 3/3 [00:00<00:00, 168.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 documents, generated 2 embeddings.\n",
      ">>>> >>>> >>>> >>>> 3. Creating collection for thenlper/gte-small w/ test_dataset_0 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_0) (5/6)\n",
      "Getting text_splitter, transformer_model, client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-specific DB test_dataset_0_256 already exists. Using it to store embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 326.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_0\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 52.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 documents, generated 13 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 56.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 documents, generated 26 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> >>>> >>>> >>>> 3. Creating collection for thenlper/gte-small w/ test_dataset_1 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_1) (6/6)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-specific DB test_dataset_1_256 already exists. Using it to store embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 423.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_1\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp5dhsws26/jsonls/test_dataset_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 3/3 [00:00<00:00, 196.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 documents, generated 1 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 3/3 [00:00<00:00, 231.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 documents, generated 2 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test that we are able to create a test dataset and ingest it into ChromaDB from text and via the StringsToJSONDataset class.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import owlergpt.modern.ingestion.ingestors\n",
    "\n",
    "\n",
    "importlib.reload(owlergpt.modern.ingestion.ingestors)  # For debugging the library\n",
    "StringsToJSONDataset = owlergpt.modern.ingestion.ingestors.StringsToJSONDataset\n",
    "OriginalIngestion = owlergpt.modern.ingestion.ingestors.OriginalIngestion\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(\"Creating dataset and regular collection\")\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=temp_dir, settings=chromadb.Settings(anonymized_telemetry=False)\n",
    "    )\n",
    "\n",
    "    collections = []\n",
    "    # Create collections and populate with embeddings\n",
    "    for i, model in tqdm(enumerate(models), desc=\"Creating collections\"):\n",
    "        # Create collection\n",
    "        # NOTE this has to be parseable\n",
    "        # `{selected_folder}_{transformer_model}_CharacterSplitting_{tokens_per_chunk}`\n",
    "        collection_name = (\n",
    "            f\"{selected_folder}_{model_names[i]}_CharacterSplitting_{tokens_per_chunk}\"\n",
    "        )\n",
    "        collection = chroma_client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\n",
    "                \"hnsw:space\": \"cosine\"\n",
    "            },  # Using cosine as default distance function\n",
    "        )\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = model.encode(test_sentences, convert_to_tensor=False)\n",
    "\n",
    "        # Create metadata for each embedding\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"record_id\": record_ids[i],\n",
    "                \"record_text\": test_sentences[i],\n",
    "                \"record_type\": \"document\",\n",
    "            }\n",
    "            for i in range(len(test_sentences))\n",
    "        ]\n",
    "\n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=test_sentences,\n",
    "            metadatas=metadatas,\n",
    "            ids=record_ids,\n",
    "        )\n",
    "\n",
    "        collections.append(collection)\n",
    "print(\"Done creating collections\")\n",
    "print(collections)\n",
    "\n",
    "\"\"\"\n",
    "Try a cartesian product of 2 datasets (each is a clone of the other one though) w/ 3 models.\n",
    "\"\"\"\n",
    "\n",
    "with tempfile.TemporaryDirectory() as _temp_dir:\n",
    "    temp_dir = Path(_temp_dir)\n",
    "    chroma_path = temp_dir / \"chroma\"\n",
    "    jsonls_path = temp_dir / \"jsonls\"\n",
    "    print(\"Using JSONDataset to embed...\")\n",
    "    print(\">>>> >>>> >>>> >>>> 1. Creating JSONL files\")\n",
    "    datasets_pre_jsonl = [\n",
    "        (test_sentences, test_queries),\n",
    "        (test_queries, test_sentences),\n",
    "    ]\n",
    "    locations = [\n",
    "        jsonls_path / f\"test_dataset_{i}\" for i in range(len(datasets_pre_jsonl))\n",
    "    ]\n",
    "    dataset_names = [f\"test_dataset_{i}\" for i in range(len(datasets_pre_jsonl))]\n",
    "    for location, (sentences, queries) in zip(\n",
    "        locations, datasets_pre_jsonl, strict=False\n",
    "    ):  # just do it twice in two ifferent folders\n",
    "        obj = StringsToJSONDataset(output_path=location)\n",
    "        obj.create_dataset(texts=sentences, queries=queries)\n",
    "    print(\n",
    "        \">>>> >>>> >>>> >>>> 2. Creating JSONL files using the default ingestion pipeline\"\n",
    "    )\n",
    "\n",
    "    total_num_collections = len(small_model_names) * len(locations)\n",
    "    i = 0\n",
    "    for model_name in small_model_names:\n",
    "        for location, dataset_name in zip(locations, dataset_names, strict=False):\n",
    "            i += 1\n",
    "            print(\n",
    "                f\">>>> >>>> >>>> >>>> 3. Creating collection for {model_name} w/ {dataset_name} (location={location}) ({i}/{total_num_collections})\"\n",
    "            )\n",
    "            chroma_client, collection = OriginalIngestion.create_collection(\n",
    "                chroma_client=None,  # use a new client every time\n",
    "                vector_dataset_path=chroma_path.as_posix(),\n",
    "                selected_folders=[dataset_name],\n",
    "                tokens_per_chunk=tokens_per_chunk,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                normalize_embeddings=False,\n",
    "                model_name=model_name,\n",
    "                batch_size=1,\n",
    "                dataset_folder_path=jsonls_path.as_posix(),\n",
    "                vector_search_chunk_prefix=\"passage: \",\n",
    "                vector_search_distance_function=\"cosine\",\n",
    "                num_workers=0,  # TODO(Adriano) WTF is going on with Mac local runtime non-cpu errors?\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JSONDataset to embed...\n",
      ">>>> >>>> >>>> >>>> 1. Creating JSONL files\n",
      ">>>> >>>> >>>> >>>> 2. Creating JSONL files using the default ingestion pipeline\n",
      ">>>> >>>> >>>> >>>> 3. Creating collection for BAAI/bge-small-en-v1.5 w/ test_dataset_0 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp_l6joizl/jsonls/test_dataset_0) (1/3)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset-specific DB test_dataset_0_256 to store embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 314.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_0\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp_l6joizl/jsonls/test_dataset_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 54.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 documents, generated 13 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 51.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 documents, generated 26 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> >>>> >>>> >>>> 3. Creating collection for intfloat/e5-small-v2 w/ test_dataset_0 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp_l6joizl/jsonls/test_dataset_0) (2/3)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-specific DB test_dataset_0_256 already exists. Using it to store embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 365.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_0\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp_l6joizl/jsonls/test_dataset_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 48.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 documents, generated 13 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 45.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 documents, generated 26 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> >>>> >>>> >>>> 3. Creating collection for thenlper/gte-small w/ test_dataset_0 (location=/var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp_l6joizl/jsonls/test_dataset_0) (3/3)\n",
      "Getting text_splitter, transformer_model, client\n",
      "Creating split embedding models\n",
      "Creating collection\n",
      "Creating 1 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-specific DB test_dataset_0_256 already exists. Using it to store embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Creating databases + collections |: 1it [00:00, 402.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset test_dataset_0\n",
      "Processing dataset /var/folders/j5/qcb5hwfx53q57rz91nm4rvr40000gn/T/tmp_l6joizl/jsonls/test_dataset_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 54.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 documents, generated 13 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "| Computing embeddings |: 100%|██████████| 11/11 [00:00<00:00, 54.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 documents, generated 26 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we try to visualize and make sure that all the metadata is indeed correct.\n",
    "\"\"\"\n",
    "\n",
    "import owlergpt.modern.ingestion.fix_metadata\n",
    "\n",
    "\n",
    "importlib.reload(owlergpt.modern.ingestion.ingestors)\n",
    "importlib.reload(owlergpt.modern.ingestion.fix_metadata)\n",
    "OriginalIngestion = owlergpt.modern.ingestion.ingestors.OriginalIngestion\n",
    "EmbeddingMetadataPopulatorArgs = (\n",
    "    owlergpt.modern.ingestion.fix_metadata.EmbeddingMetadataPopulatorArgs\n",
    ")\n",
    "EmbeddingMetadataPopulatorCoordinator = (\n",
    "    owlergpt.modern.ingestion.fix_metadata.EmbeddingMetadataPopulatorCoordinator\n",
    ")\n",
    "EmbeddingMetadataPopulator = (\n",
    "    owlergpt.modern.ingestion.fix_metadata.EmbeddingMetadataPopulator\n",
    ")\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as _temp_dir:\n",
    "    temp_dir = Path(_temp_dir)\n",
    "    chroma_path = temp_dir / \"chroma\"\n",
    "    jsonls_path = temp_dir / \"jsonls\"\n",
    "    print(\"Using JSONDataset to embed...\")\n",
    "    print(\">>>> >>>> >>>> >>>> 1. Creating JSONL files\")\n",
    "    datasets_pre_jsonl = [\n",
    "        (test_sentences, test_queries)\n",
    "    ]  # NOTE <---- one dataset (as we will do) but multiple models => x3\n",
    "    locations = [\n",
    "        jsonls_path / f\"test_dataset_{i}\" for i in range(len(datasets_pre_jsonl))\n",
    "    ]\n",
    "    dataset_names = [f\"test_dataset_{i}\" for i in range(len(datasets_pre_jsonl))]\n",
    "    for location, (sentences, queries) in zip(\n",
    "        locations, datasets_pre_jsonl, strict=False\n",
    "    ):  # just do it twice in two ifferent folders\n",
    "        obj = StringsToJSONDataset(output_path=location)\n",
    "        obj.create_dataset(texts=sentences, queries=queries)\n",
    "    print(\n",
    "        \">>>> >>>> >>>> >>>> 2. Creating JSONL files using the default ingestion pipeline\"\n",
    "    )\n",
    "\n",
    "    total_num_collections = len(small_model_names) * len(locations)\n",
    "    i = 0\n",
    "\n",
    "    # Create collections\n",
    "    location = locations[0]\n",
    "    dataset_name = dataset_names[0]\n",
    "    assert len(locations) == len(dataset_names) == 1\n",
    "\n",
    "    collections: list[chromadb.Collection] = []\n",
    "    chroma_client = None\n",
    "    for model_name in small_model_names:\n",
    "        i += 1\n",
    "        print(\n",
    "            f\">>>> >>>> >>>> >>>> 3. Creating collection for {model_name} w/ {dataset_name} (location={location}) ({i}/{total_num_collections})\"\n",
    "        )\n",
    "        chroma_client, collection = OriginalIngestion.create_collection(\n",
    "            chroma_client=chroma_client,  # <--- try reusing the client\n",
    "            vector_dataset_path=chroma_path.as_posix(),\n",
    "            selected_folders=[dataset_name],\n",
    "            tokens_per_chunk=tokens_per_chunk,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            normalize_embeddings=False,\n",
    "            model_name=model_name,\n",
    "            batch_size=1,\n",
    "            dataset_folder_path=jsonls_path.as_posix(),\n",
    "            vector_search_chunk_prefix=\"passage: \",\n",
    "            vector_search_distance_function=\"cosine\",\n",
    "            num_workers=0,  # TODO(Adriano) WTF is going on with Mac local runtime non-cpu errors?\n",
    "        )\n",
    "        collections.append(collection)\n",
    "    print(\"We have a client: \", chroma_client)\n",
    "    print(\"We have collections: \", collections)\n",
    "\n",
    "    print(\">>>> >>>> >>>> >>>> 4. Visualizing the metadata FML\")\n",
    "    raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    print(\">>>> >>>> >>>> >>>> 5. Testing metadata correction\")\n",
    "    raise NotImplementedError(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
