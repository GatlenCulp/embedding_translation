{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generated by O1 and modified. This is meant to help make sure that we aren't fucking up embeddings. For each dataset, for each pair of models\n",
    "the embeddings should NOT be the same. At he same time, the chunks should be the same as should the IDs and everything else.\n",
    "\"\"\"\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "from safetensors import safe_open\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATASETS = [\n",
    "    # (numbers are counts for documents, there may be some longer documents -> slightly more chunks)\n",
    "    \"arguana\",  # 10K\n",
    "    \"fiqa\",  # 50K -> 20K\n",
    "    \"scidocs\",  # 25K -> 20K\n",
    "    \"nfcorpus\",  # 5K\n",
    "    \"hotpotqa\",  # 100K -> 20K\n",
    "    \"trec-covid\",  # too much -> 20K\n",
    "]\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"Salesforce/SFR-Embedding-Mistral\",\n",
    "    \"WhereIsAI/UAE-Large-V1\",\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    \"intfloat/e5-base-v2\",\n",
    "    \"intfloat/e5-large-v2\",\n",
    "    \"intfloat/e5-small-v2\",\n",
    "    \"thenlper/gte-base\",\n",
    "    \"thenlper/gte-large\",\n",
    "    \"thenlper/gte-small\",\n",
    "    \"sentence-transformers/gtr-t5-base\",\n",
    "    \"sentence-transformers/gtr-t5-large\",\n",
    "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    \"sentence-transformers/sentence-t5-base\",\n",
    "    \"sentence-transformers/sentence-t5-large\",\n",
    "    \"text-embedding-3-large\",  # openai\n",
    "    \"text-embedding-3-small\",  # openai\n",
    "]\n",
    "# MODEL_NAMES = [m.replace(\"/\", \"_\") for m in MODEL_NAMES] # we store them this way\n",
    "\n",
    "\n",
    "# NOTE: copied from chunk_dataset.py and elsewhere\n",
    "class Chunk(BaseModel):\n",
    "    id: str = Field(alias=\"id\")\n",
    "    doc_id: str = Field(alias=\"doc_id\")\n",
    "    index_in_doc: int = Field(alias=\"index_in_doc\")\n",
    "    text: str = Field(alias=\"text\")\n",
    "\n",
    "\n",
    "def load_safetensors_embeddings(filepath: Path) -> torch.Tensor:\n",
    "    if not filepath.exists():\n",
    "        return None\n",
    "    with safe_open(filepath.as_posix(), framework=\"pt\", device=\"cpu\") as f:\n",
    "        # Should contain a single key \"embeddings\"\n",
    "        return f.get_tensor(\"embeddings\")\n",
    "\n",
    "\n",
    "def load_metadata(filepath: Path) -> list[Chunk]:\n",
    "    with open(filepath) as f:\n",
    "        return [Chunk.model_validate_json(line) for line in f if len(line.strip()) > 0]\n",
    "\n",
    "\n",
    "def compare_metadata(meta1: list[Chunk], meta2: list[Chunk]) -> bool:\n",
    "    \"\"\"Return if metadata is the same.\"\"\"\n",
    "    if len(meta1) != len(meta2):\n",
    "        return False\n",
    "    for c1, c2 in zip(meta1, meta2, strict=False):\n",
    "        if (\n",
    "            c1.id != c2.id\n",
    "            or c1.doc_id != c2.doc_id\n",
    "            or c1.index_in_doc != c2.index_in_doc\n",
    "            or c1.text != c2.text\n",
    "        ):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def model2model_dimension(model_name: str) -> int:\n",
    "    \"\"\"Helper: get the size of the embedding dimension vector (1D, usually something like 768-4096).\"\"\"\n",
    "    # Miscellaneous (HF)\n",
    "    if \"/\" in model_name:\n",
    "        assert model_name.count(\"/\") == 1\n",
    "        model_name = model_name.split(\"/\")[-1]\n",
    "    if model_name == \"SFR-Embedding-Mistral\":\n",
    "        return 4096\n",
    "    if model_name == \"UAE-Large-V1\" or model_name == \"mxbai-embed-large-v1\":\n",
    "        return 1024\n",
    "    # BGE Models (HF)\n",
    "    if model_name == \"bge-base-en-v1.5\":\n",
    "        return 768\n",
    "    if model_name == \"bge-large-en-v1.5\":\n",
    "        return 1024\n",
    "    if model_name == \"bge-small-en-v1.5\":\n",
    "        return 384\n",
    "    #  E5 Models (HF)\n",
    "    if model_name == \"e5-base-v2\":\n",
    "        return 768\n",
    "    if model_name == \"e5-large-v2\":\n",
    "        return 1024\n",
    "    if model_name == \"e5-small-v2\":\n",
    "        return 384\n",
    "    # GTE Models (HF)\n",
    "    if model_name == \"gte-base\":\n",
    "        return 768\n",
    "    if model_name == \"gte-large\":\n",
    "        return 1024\n",
    "    if model_name == \"gte-small\":\n",
    "        return 384\n",
    "    # GTR-T5 Models (HF)\n",
    "    if (\n",
    "        model_name == \"gtr-t5-base\"\n",
    "        or model_name == \"gtr-t5-large\"\n",
    "        or model_name == \"sentence-t5-base\"\n",
    "        or model_name == \"sentence-t5-large\"\n",
    "    ):\n",
    "        return 768\n",
    "    # OpenAI Models\n",
    "    if model_name == \"text-embedding-3-large\":\n",
    "        return 3072\n",
    "    if model_name == \"text-embedding-3-small\":\n",
    "        return 1536\n",
    "    # NOTE: cohere may be supported in THE FUTURE\n",
    "    raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "\n",
    "def get_model_files(model_dir: Path) -> list[Path]:\n",
    "    return [\n",
    "        # embeddings\n",
    "        model_dir / \"embeddings_corpus_train.safetensors\",\n",
    "        model_dir / \"embeddings_corpus_validation.safetensors\",\n",
    "        model_dir / \"embeddings_queries_train.safetensors\",\n",
    "        model_dir / \"embeddings_queries_validation.safetensors\",\n",
    "        # metadata\n",
    "        model_dir / \"metadatas_corpus_train.jsonl\",\n",
    "        model_dir / \"metadatas_corpus_validation.jsonl\",\n",
    "        model_dir / \"metadatas_queries_train.jsonl\",\n",
    "        model_dir / \"metadatas_queries_validation.jsonl\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_reversed_model_files(model_dir: Path) -> list[Path]:\n",
    "    return [\n",
    "        # reverse because sometimes we do \"corpus_embeddings...\"\n",
    "        #\n",
    "        # saftensors\n",
    "        model_dir / \"corpus_train_embeddings.safetensors\",\n",
    "        model_dir / \"queries_train_embeddings.safetensors\",\n",
    "        model_dir / \"corpus_validation_embeddings.safetensors\",\n",
    "        model_dir / \"queries_validation_embeddings.safetensors\",\n",
    "        # jsonls\n",
    "        model_dir / \"corpus_train_metadatas.jsonl\",\n",
    "        model_dir / \"queries_train_metadatas.jsonl\",\n",
    "        model_dir / \"corpus_validation_metadatas.jsonl\",\n",
    "        model_dir / \"queries_validation_metadatas.jsonl\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sure_individually_ok(root_dir: Path) -> list[tuple[str, str, str]]:\n",
    "    skipped_model_pairs: list[\n",
    "        tuple[str, str, str]\n",
    "    ] = []  # (dataset_name, model_name, reason)\n",
    "    cartesian_product = list(itertools.product(DATASETS, MODEL_NAMES))\n",
    "    for dataset, model_name in tqdm(cartesian_product):\n",
    "        model_dir = root_dir / model_name.replace(\"/\", \"_\") / dataset\n",
    "        if not model_dir.exists():\n",
    "            skipped_model_pairs.append((dataset, model_name, \"missing dir\"))\n",
    "            continue\n",
    "        model_files = get_model_files(model_dir)\n",
    "        tensors_files = model_files[:4]\n",
    "        meta_files = model_files[4:]\n",
    "        if any(not f.exists() for f in model_files):\n",
    "            # exactly subset of\n",
    "            # ├── corpus_embeddings.safetensors\n",
    "            # ├── corpus_metadatas.jsonl\n",
    "            # ├── corpus_train_embeddings.safetensors\n",
    "            # ├── corpus_train_metadatas.jsonl\n",
    "            # ├── corpus_validation_embeddings.safetensors\n",
    "            # ├── corpus_validation_metadatas.jsonl\n",
    "            # ├── queries_embeddings.safetensors\n",
    "            # ├── queries_metadatas.jsonl\n",
    "            # ├── queries_train_embeddings.safetensors\n",
    "            # ├── queries_train_metadatas.jsonl\n",
    "            # ├── queries_validation_embeddings.safetensors\n",
    "            # └── queries_validation_metadatas.jsonl\n",
    "            model_files = get_reversed_model_files(model_dir)\n",
    "            if any(not f.exists() for f in model_files):\n",
    "                skipped_model_pairs.append((dataset, model_name, \"missing files\"))\n",
    "                continue\n",
    "            tensors_files = model_files[:4]\n",
    "            meta_files = model_files[4:]\n",
    "        else:\n",
    "            # If you make it here you should not be skipped and should be good if you make it through\n",
    "            for f_tensor, f_meta in zip(tensors_files, meta_files, strict=False):\n",
    "                tensors = load_safetensors_embeddings(f_tensor)\n",
    "                meta = load_metadata(f_meta)\n",
    "                assert len(tensors.shape) == 2\n",
    "                assert len(meta) == tensors.shape[0]\n",
    "                expected_length = model2model_dimension(model_name)\n",
    "                assert tensors.shape[1] == expected_length\n",
    "    return skipped_model_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR1 = Path(\"/mnt/align3_drive/adrianoh/dl_final_project_embeddings_huggingface\")\n",
    "ROOT_DIR2 = Path(\"/mnt/align3_drive/adrianoh/dl_final_project_embeddings_openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:17<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'missing dir')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'missing dir')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'missing dir')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'missing dir')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'missing dir')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'missing dir')\n",
      "('trec-covid', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "0.17592592592592593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = make_sure_individually_ok(ROOT_DIR1)\n",
    "print(\n",
    "    \"\\n\".join(map(str, [x for x in x if \"text-embedding-3-\" not in x[1]]))\n",
    ")  # huggingface\n",
    "print(len(x) / len(DATASETS) / len(MODEL_NAMES))  # pct -> want to be close to zero ngl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 2858.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = make_sure_individually_ok(ROOT_DIR2)\n",
    "printout = \"\\n\".join(map(str, [x for x in x if \"text-embedding-3-\" in x[1]]))  # openai\n",
    "if \"text-embedding-3-\" in printout:\n",
    "    print(\"WTF\")\n",
    "print(printout)  # should be EMPTY no matter what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check all pairs\n",
    "def make_sure_pairs_ok(\n",
    "    root_dir: Path,\n",
    "    stream: bool = False,\n",
    "    filter_against: str = None,\n",
    "    filter_for: str = None,\n",
    ") -> tuple[list[tuple[str, str, str, str]], list[tuple[str, str, str, str]]]:\n",
    "    skipped_model_pairs: list[\n",
    "        tuple[str, str, str, str]\n",
    "    ] = []  # (dataset_name, model1_name, model2_name, reason)\n",
    "    bad_model_pairs: list[\n",
    "        tuple[str, str, str, str]\n",
    "    ] = []  # (dataset_name, model1_name, model2_name, reason)\n",
    "    cartesian_product = list(itertools.product(DATASETS, MODEL_NAMES, MODEL_NAMES))\n",
    "    cartesian_product = [\n",
    "        x for x in cartesian_product if x[1] != x[2]\n",
    "    ]  # don't compare the same model to itself\n",
    "    if stream:\n",
    "        print(\"Filtering against: \", filter_against)\n",
    "        print(\"Filtering for: \", filter_for)\n",
    "        print(\"Starting on \", len(cartesian_product), \" pairs\")\n",
    "    cartesian_product = [\n",
    "        x\n",
    "        for x in cartesian_product\n",
    "        if (\n",
    "            filter_against is None\n",
    "            or not (filter_against in x[1] or filter_against in x[2])\n",
    "        )\n",
    "    ]\n",
    "    cartesian_product = [\n",
    "        x\n",
    "        for x in cartesian_product\n",
    "        if (filter_for is None or (filter_for in x[1] and filter_for in x[2]))\n",
    "    ]\n",
    "    if stream:\n",
    "        print(\"Reduced to \", len(cartesian_product), \" pairs\")\n",
    "    for dataset_name, model1_name, model2_name in tqdm(cartesian_product):\n",
    "        assert model1_name != model2_name\n",
    "        model1_path = root_dir / model1_name.replace(\"/\", \"_\") / dataset_name\n",
    "        model2_path = root_dir / model2_name.replace(\"/\", \"_\") / dataset_name\n",
    "        # NOTE we need something like these:\n",
    "        # scidocs\n",
    "        # │   │   ├── embeddings_corpus_train.safetensors\n",
    "        # │   │   ├── embeddings_corpus_validation.safetensors\n",
    "        # │   │   ├── embeddings_queries_train.safetensors\n",
    "        # │   │   ├── embeddings_queries_validation.safetensors\n",
    "        # │   │   ├── metadatas_corpus_train.jsonl\n",
    "        # │   │   ├── metadatas_corpus_validation.jsonl\n",
    "        # │   │   ├── metadatas_queries_train.jsonl\n",
    "        # │   │   └── metadatas_queries_validation.jsonl\n",
    "        model1_files = get_model_files(model1_path)\n",
    "        if any(not f.exists() for f in model1_files):\n",
    "            model1_files = get_reversed_model_files(model1_path)\n",
    "        model2_files = get_model_files(model2_path)\n",
    "        if any(not f.exists() for f in model2_files):\n",
    "            model2_files = get_reversed_model_files(model2_path)\n",
    "        if any(not f.exists() for f in model1_files) or any(\n",
    "            not f.exists() for f in model2_files\n",
    "        ):\n",
    "            skipped_model_pairs.append(\n",
    "                (dataset_name, model1_name, model2_name, \"missing files\")\n",
    "            )\n",
    "            continue\n",
    "        assert len(model1_files) == len(model2_files)\n",
    "        # Make sure that the pairs are OK\n",
    "        for file1, file2 in zip(model1_files, model2_files, strict=False):\n",
    "            assert file1.exists() and file2.exists()\n",
    "            assert file1.name == file2.name\n",
    "            # 1. Make sure all models are different embedding\n",
    "            if file1.suffix == \".safetensors\":\n",
    "                tensors1 = load_safetensors_embeddings(file1)\n",
    "                tensors2 = load_safetensors_embeddings(file2)\n",
    "                assert tensors1 is not None and tensors2 is not None\n",
    "                if tensors1.shape == tensors2.shape and torch.allclose(\n",
    "                    tensors1, tensors2\n",
    "                ):  # should be DIFFERENT\n",
    "                    if stream:\n",
    "                        print(\n",
    "                            \"BAD [EMBEDDINGS]: \",\n",
    "                            dataset_name,\n",
    "                            model1_name,\n",
    "                            model2_name,\n",
    "                        )\n",
    "                    bad_model_pairs.append(\n",
    "                        (\n",
    "                            dataset_name,\n",
    "                            model1_name,\n",
    "                            model2_name,\n",
    "                            \"embeddings all close\",\n",
    "                        )\n",
    "                    )\n",
    "            # 2. Make sure that all metadatas match though\n",
    "            elif file1.suffix == \".jsonl\":\n",
    "                meta1 = load_metadata(file1)\n",
    "                meta2 = load_metadata(file2)\n",
    "                assert meta1 is not None and meta2 is not None\n",
    "                if not compare_metadata(meta1, meta2):  # should be SAME\n",
    "                    if stream:\n",
    "                        print(\n",
    "                            \"BAD [METADATA]: \",\n",
    "                            dataset_name,\n",
    "                            model1_name,\n",
    "                            model2_name,\n",
    "                        )\n",
    "                    bad_model_pairs.append(\n",
    "                        (\n",
    "                            dataset_name,\n",
    "                            model1_name,\n",
    "                            model2_name,\n",
    "                            \"metadata not same\",\n",
    "                        )\n",
    "                    )\n",
    "    return skipped_model_pairs, bad_model_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering against:  text-embedding-3\n",
      "Filtering for:  None\n",
      "Starting on  1836  pairs\n",
      "Reduced to  1440  pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [09:00<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== SKIPPED ==================================================\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'WhereIsAI/UAE-Large-V1', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-base-en-v1.5', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-large-en-v1.5', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-small-en-v1.5', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-base-v2', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-large-v2', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-small-v2', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-base', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-large', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-small', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-base', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-large', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'mixedbread-ai/mxbai-embed-large-v1', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-base', 'missing files')\n",
      "('arguana', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "('arguana', 'WhereIsAI/UAE-Large-V1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'BAAI/bge-base-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'BAAI/bge-large-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'BAAI/bge-small-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'intfloat/e5-base-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'intfloat/e5-large-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'intfloat/e5-small-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'thenlper/gte-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'thenlper/gte-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'thenlper/gte-small', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'sentence-transformers/gtr-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'sentence-transformers/gtr-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'mixedbread-ai/mxbai-embed-large-v1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'sentence-transformers/sentence-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('arguana', 'sentence-transformers/sentence-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'WhereIsAI/UAE-Large-V1', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-base-en-v1.5', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-large-en-v1.5', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-small-en-v1.5', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-base-v2', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-large-v2', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-small-v2', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-base', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-large', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-small', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-base', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-large', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'mixedbread-ai/mxbai-embed-large-v1', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-base', 'missing files')\n",
      "('fiqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "('fiqa', 'WhereIsAI/UAE-Large-V1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'BAAI/bge-base-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'BAAI/bge-large-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'BAAI/bge-small-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'intfloat/e5-base-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'intfloat/e5-large-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'intfloat/e5-small-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'thenlper/gte-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'thenlper/gte-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'thenlper/gte-small', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'sentence-transformers/gtr-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'sentence-transformers/gtr-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'mixedbread-ai/mxbai-embed-large-v1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'sentence-transformers/sentence-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('fiqa', 'sentence-transformers/sentence-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'WhereIsAI/UAE-Large-V1', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-base-en-v1.5', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-large-en-v1.5', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-small-en-v1.5', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-base-v2', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-large-v2', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-small-v2', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-base', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-large', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-small', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-base', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-large', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'mixedbread-ai/mxbai-embed-large-v1', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-base', 'missing files')\n",
      "('scidocs', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "('scidocs', 'WhereIsAI/UAE-Large-V1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'BAAI/bge-base-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'BAAI/bge-large-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'BAAI/bge-small-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'intfloat/e5-base-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'intfloat/e5-large-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'intfloat/e5-small-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'thenlper/gte-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'thenlper/gte-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'thenlper/gte-small', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'sentence-transformers/gtr-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'sentence-transformers/gtr-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'mixedbread-ai/mxbai-embed-large-v1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'sentence-transformers/sentence-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('scidocs', 'sentence-transformers/sentence-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'WhereIsAI/UAE-Large-V1', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-base-en-v1.5', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-large-en-v1.5', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-small-en-v1.5', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-base-v2', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-large-v2', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-small-v2', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-base', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-large', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-small', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-base', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-large', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'mixedbread-ai/mxbai-embed-large-v1', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-base', 'missing files')\n",
      "('nfcorpus', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "('nfcorpus', 'WhereIsAI/UAE-Large-V1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'BAAI/bge-base-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'BAAI/bge-large-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'BAAI/bge-small-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'intfloat/e5-base-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'intfloat/e5-large-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'intfloat/e5-small-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'thenlper/gte-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'thenlper/gte-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'thenlper/gte-small', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'sentence-transformers/gtr-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'sentence-transformers/gtr-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'mixedbread-ai/mxbai-embed-large-v1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'sentence-transformers/sentence-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('nfcorpus', 'sentence-transformers/sentence-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'WhereIsAI/UAE-Large-V1', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-base-en-v1.5', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-large-en-v1.5', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-small-en-v1.5', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-base-v2', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-large-v2', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-small-v2', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-base', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-large', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-small', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-base', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-large', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'mixedbread-ai/mxbai-embed-large-v1', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-base', 'missing files')\n",
      "('hotpotqa', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "('hotpotqa', 'WhereIsAI/UAE-Large-V1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'BAAI/bge-base-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'BAAI/bge-large-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'BAAI/bge-small-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'intfloat/e5-base-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'intfloat/e5-large-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'intfloat/e5-small-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'thenlper/gte-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'thenlper/gte-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'thenlper/gte-small', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'sentence-transformers/gtr-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'sentence-transformers/gtr-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'mixedbread-ai/mxbai-embed-large-v1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'sentence-transformers/sentence-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('hotpotqa', 'sentence-transformers/sentence-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'WhereIsAI/UAE-Large-V1', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-base-en-v1.5', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-large-en-v1.5', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'BAAI/bge-small-en-v1.5', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-base-v2', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-large-v2', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'intfloat/e5-small-v2', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-base', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-large', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'thenlper/gte-small', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-base', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/gtr-t5-large', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'mixedbread-ai/mxbai-embed-large-v1', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-base', 'missing files')\n",
      "('trec-covid', 'Salesforce/SFR-Embedding-Mistral', 'sentence-transformers/sentence-t5-large', 'missing files')\n",
      "('trec-covid', 'WhereIsAI/UAE-Large-V1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'BAAI/bge-base-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'BAAI/bge-large-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'BAAI/bge-small-en-v1.5', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'intfloat/e5-base-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'intfloat/e5-large-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'intfloat/e5-small-v2', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'thenlper/gte-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'thenlper/gte-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'thenlper/gte-small', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'sentence-transformers/gtr-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'sentence-transformers/gtr-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'mixedbread-ai/mxbai-embed-large-v1', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'sentence-transformers/sentence-t5-base', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "('trec-covid', 'sentence-transformers/sentence-t5-large', 'Salesforce/SFR-Embedding-Mistral', 'missing files')\n",
      "================================================== BAD ==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skipped, bad = make_sure_pairs_ok(\n",
    "    ROOT_DIR1, stream=True, filter_against=\"text-embedding-3\"\n",
    ")  # very low so we stream\n",
    "print(\"=\" * 50 + \" SKIPPED \" + \"=\" * 50)\n",
    "print(\n",
    "    \"\\n\".join(\n",
    "        map(str, [s for s in skipped if \"text-embedding-3-\" not in (s[1] + s[2])])\n",
    "    )\n",
    ")  # concenate is an easy hack for this\n",
    "print(\"=\" * 50 + \" BAD \" + \"=\" * 50)\n",
    "print(\"\\n\".join(map(str, [s for s in bad if \"text-embedding-3-\" not in (s[1] + s[2])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:04<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== SKIPPED ==================================================\n",
      "\n",
      "================================================== BAD ==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skipped, bad = make_sure_pairs_ok(ROOT_DIR2, filter_for=\"text-embedding-3\")\n",
    "print(\"=\" * 50 + \" SKIPPED \" + \"=\" * 50)\n",
    "print(\"\\n\".join(map(str, [s for s in skipped if \"text-embedding-3-\" in (s[1] + s[2])])))\n",
    "print(\"=\" * 50 + \" BAD \" + \"=\" * 50)\n",
    "print(\"\\n\".join(map(str, [s for s in bad if \"text-embedding-3-\" in (s[1] + s[2])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
