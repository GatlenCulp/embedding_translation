{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intfloat_e5-small-v2 : True\n",
      "intfloat_e5-large-v2 : True\n",
      "sentence-transformers_sentence-t5-large : True\n",
      "thenlper_gte-small : True\n",
      "Salesforce_SFR-Embedding-Mistral : True\n",
      "BAAI_bge-large-en-v1.5 : True\n",
      "intfloat_e5-base-v2 : True\n",
      "BAAI_bge-base-en-v1.5 : True\n",
      "sentence-transformers_gtr-t5-large : True\n",
      "thenlper_gte-base : True\n",
      "sentence-transformers_sentence-t5-base : True\n",
      "thenlper_gte-large : True\n",
      "WhereIsAI_UAE-Large-V1 : True\n",
      "BAAI_bge-small-en-v1.5 : True\n",
      "sentence-transformers_gtr-t5-base : True\n",
      "mixedbread-ai_mxbai-embed-large-v1 : True\n",
      "True\n",
      "=====\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Got 120 embeddings (total n_embeddings=16) that are the same:\n\n{\n  \"intfloat_e5-small-v2 <|> intfloat_e5-large-v2\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-small-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-small-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-large-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-large-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-small\": true,\n  \"sentence-transformers_sentence-t5-large <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-large-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> intfloat_e5-base-v2\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-base-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-small <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"thenlper_gte-small <|> BAAI_bge-large-en-v1.5\": true,\n  \"thenlper_gte-small <|> intfloat_e5-base-v2\": true,\n  \"thenlper_gte-small <|> BAAI_bge-base-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-large\": true,\n  \"thenlper_gte-small <|> thenlper_gte-base\": true,\n  \"thenlper_gte-small <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-small <|> thenlper_gte-large\": true,\n  \"thenlper_gte-small <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-small <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-small <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-large-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> intfloat_e5-base-v2\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-base-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_sentence-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> WhereIsAI_UAE-Large-V1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-small-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> intfloat_e5-base-v2\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-base-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-base-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_gtr-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_gtr-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-base <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-base <|> thenlper_gte-large\": true,\n  \"thenlper_gte-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-base <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> BAAI_bge-small-en-v1.5\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> sentence-transformers_gtr-t5-base\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-small-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-small-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 104\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     95\u001b[0m             EmbeddingDataset(\n\u001b[1;32m     96\u001b[0m                 embeddings\u001b[38;5;241m=\u001b[39membeddings_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m embeddings_set, ids_set, documents_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embeddings_sets, ids_sets, documents_sets)\n\u001b[1;32m    101\u001b[0m         ]\n\u001b[1;32m    103\u001b[0m loader \u001b[38;5;241m=\u001b[39m EmbeddingLoader(root_path\u001b[38;5;241m=\u001b[39mARGUANA_PATH)\n\u001b[0;32m--> 104\u001b[0m embeddings_list: List[EmbeddingDataset] \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_folders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get all the document/corpus embeddings\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings shape is\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m= ...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# axis 0 is dataset, axis 1 is embedding\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings devices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings1.device\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings2.device\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metc...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# shoulds be cpu cpu\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 93\u001b[0m, in \u001b[0;36mEmbeddingLoader.fetch_embeddings\u001b[0;34m(self, corpus_folder, model_folders)\u001b[0m\n\u001b[1;32m     91\u001b[0m is_close \u001b[38;5;241m=\u001b[39m {x : y \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m is_close\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m y}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# raise NotImplementedError() # XXX\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(is_close) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(is_close)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings (total n_embeddings=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings_sets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that are the same:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(is_close,\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     95\u001b[0m     EmbeddingDataset(\n\u001b[1;32m     96\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m embeddings_set, ids_set, documents_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embeddings_sets, ids_sets, documents_sets)\n\u001b[1;32m    101\u001b[0m ]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Got 120 embeddings (total n_embeddings=16) that are the same:\n\n{\n  \"intfloat_e5-small-v2 <|> intfloat_e5-large-v2\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-small-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-small-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-large-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-large-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-small\": true,\n  \"sentence-transformers_sentence-t5-large <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-large-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> intfloat_e5-base-v2\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-base-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-small <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"thenlper_gte-small <|> BAAI_bge-large-en-v1.5\": true,\n  \"thenlper_gte-small <|> intfloat_e5-base-v2\": true,\n  \"thenlper_gte-small <|> BAAI_bge-base-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-large\": true,\n  \"thenlper_gte-small <|> thenlper_gte-base\": true,\n  \"thenlper_gte-small <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-small <|> thenlper_gte-large\": true,\n  \"thenlper_gte-small <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-small <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-small <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-large-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> intfloat_e5-base-v2\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-base-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_sentence-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> WhereIsAI_UAE-Large-V1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-small-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> intfloat_e5-base-v2\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-base-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-base-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_gtr-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_gtr-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-base <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-base <|> thenlper_gte-large\": true,\n  \"thenlper_gte-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-base <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> BAAI_bge-small-en-v1.5\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> sentence-transformers_gtr-t5-base\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-small-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-small-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true\n}"
     ]
    }
   ],
   "source": [
    "# XXX XXX WHAT THE FUCK IS GOING ON WHAT THE FUCK IS GOING ON WHAT THE FUCK IS GOING ON XXX XXX\n",
    "import torch\n",
    "import safetensors\n",
    "import pydantic\n",
    "import safetensors.torch\n",
    "from typing import Literal, Optional, List, Dict, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "DATASETS_PATH = Path(\"/mnt/align3_drive/adrianoh/dl_final_project_embeddings\")\n",
    "NFCORPUS_PATH = DATASETS_PATH / \"nfcorpus-real-other-is-scidocs\"\n",
    "SCIDOCS_PATH = DATASETS_PATH / \"nfcorpus\"\n",
    "FIQA_PATH = DATASETS_PATH / \"fiqa\"\n",
    "ARGUANA_PATH = DATASETS_PATH / \"arguana\"\n",
    "\n",
    "class EmbeddingDataset(pydantic.BaseModel):\n",
    "    config: pydantic.ConfigDict = pydantic.ConfigDict(arbitrary_types_allowed=True)\n",
    "    embeddings: torch.Tensor\n",
    "    ids: List[str]\n",
    "    documents: List[str]\n",
    "\n",
    "class EmbeddingLoader:\n",
    "    def __init__(self, root_path: Path):\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def fetch_embeddings(\n",
    "        self,\n",
    "        corpus_folder: Literal[\"corpus\", \"query\"] = \"corpus\",\n",
    "        model_folders: Optional[List[str]] = None\n",
    "    ) -> List[EmbeddingDataset]:\n",
    "        \"\"\"\n",
    "        Fetch the set of embeddings for the given corpus and subfolders.\n",
    "        If it's `None` for `model_folders`, it will fetch all the models' embeddings.\n",
    "        \"\"\"\n",
    "        if model_folders is None:\n",
    "            model_folders = [model_folder.name for model_folder in (self.root_path / corpus_folder).iterdir() if model_folder.is_dir()] # fmt: skip\n",
    "        subpaths = [(self.root_path / corpus_folder / model_folder) for model_folder in model_folders]\n",
    "        print('\\n'.join(f\"{subpath.name} : {subpath.exists()}\" for subpath in subpaths))\n",
    "        embedding1 = safetensors.torch.load_file(subpaths[0] / \"embeddings.safetensors\", device=\"cpu\")\n",
    "        embedding2 = safetensors.torch.load_file(subpaths[1] / \"embeddings.safetensors\", device=\"cpu\")\n",
    "        embedding1 = embedding1[\"embeddings\"]\n",
    "        embedding2 = embedding2[\"embeddings\"]\n",
    "        print(torch.allclose(embedding1, embedding2))\n",
    "        print(\"=====\")# XXX\n",
    "        assert len(set(subpath.name for subpath in subpaths)) == len(subpaths)\n",
    "        # The files we want\n",
    "        embeddings_paths = [subpath / \"embeddings.safetensors\" for subpath in subpaths]\n",
    "        ids_paths = [subpath / \"ids.jsonl\" for subpath in subpaths]\n",
    "        documents_paths = [subpath / \"documents.jsonl\" for subpath in subpaths]\n",
    "\n",
    "        # They should all have the correct files present\n",
    "        assert all(embeddings_path.exists() for embeddings_path in embeddings_paths), (\"Not all embeddings paths exist:\\n  \" + '\\n  '.join(f'{p.as_posix()}: {p.exists()}' for p in embeddings_paths)) # fmt: skip\n",
    "        assert all(ids_path.exists() for ids_path in ids_paths), f\"Not all ids paths exist: {ids_paths}\"\n",
    "        assert all(documents_path.exists() for documents_path in documents_paths), f\"Not all documents paths exist: {documents_paths}\" # fmt: skip\n",
    "\n",
    "        _embeddings_sets: List[Dict[str, torch.Tensor]] = [safetensors.torch.load_file(embeddings_path, device=\"cpu\") for embeddings_path in embeddings_paths] # fmt: skip\n",
    "        _ids_sets: List[Dict[str, List[str]]] = [json.load(open(ids_path, \"r\")) for ids_path in ids_paths]\n",
    "        _documents_sets: List[Dict[str, List[str]]] = [json.load(open(documents_path, \"r\")) for documents_path in documents_paths] # fmt: skip\n",
    "\n",
    "        # Extrat the jsons\n",
    "        assert all(isinstance(ids_set, dict) and \"ids\" in ids_set and len(ids_set) == 1 for ids_set in _ids_sets)\n",
    "        assert all(isinstance(documents_set, dict) and \"documents\" in documents_set and len(documents_set) == 1 for documents_set in _documents_sets) # fmt: skip\n",
    "        assert all(isinstance(embeddings_set, dict) and \"embeddings\" in embeddings_set and len(embeddings_set) == 1 for embeddings_set in _embeddings_sets) # fmt: skip\n",
    "        ids_sets: List[List[str]] = [ids_set[\"ids\"] for ids_set in _ids_sets]\n",
    "        documents_sets: List[List[str]] = [documents_set[\"documents\"] for documents_set in _documents_sets] # fmt: skip\n",
    "        embeddings_sets: List[torch.Tensor] = [embeddings_set[\"embeddings\"] for embeddings_set in _embeddings_sets] # fmt: skip\n",
    "\n",
    "        # Make sure that all these have the same lengths\n",
    "        _ids_lengths = [len(ids_set) for ids_set in ids_sets]\n",
    "        _documents_lengths = [len(documents_set) for documents_set in documents_sets] # fmt: skip\n",
    "        _embeddings_lengths = [len(embeddings_set) for embeddings_set in embeddings_sets] # fmt: skip\n",
    "        assert len(set(_ids_lengths)) == 1\n",
    "        assert len(set(_documents_lengths)) == 1\n",
    "        assert len(set(_embeddings_lengths)) == 1\n",
    "\n",
    "        # Make sure the types are OK => they come from the same dataset but different models embedded so they should ONLY differ in that\n",
    "        assert all(isinstance(ids_set, list) and all(isinstance(id, str) for id in ids_set) for ids_set in ids_sets) # fmt: skip\n",
    "        assert all(ids_set1 == ids_sets[0] for ids_set1 in ids_sets) # star pattern for equality\n",
    "        # ...\n",
    "        assert all(isinstance(documents_set, list) and all(isinstance(doc, str) for doc in documents_set) for documents_set in documents_sets) # fmt: skip\n",
    "        assert all(documents_set1 == documents_sets[0] for documents_set1 in documents_sets) # star pattern for equality\n",
    "        # ...\n",
    "        assert all(isinstance(embeddings_set, torch.Tensor) and embeddings_set.shape == embeddings_sets[0].shape for embeddings_set in embeddings_sets) # fmt: skip\n",
    "        # Cartesian product: all most be unique\n",
    "        # avoid double-counting and self-comparison\n",
    "        is_close: Dict[str, bool] = {}\n",
    "        for i in range(len(embeddings_sets)):\n",
    "            for j in range(i + 1, len(embeddings_sets)):\n",
    "                path1, path2 = embeddings_paths[i].parent.name, embeddings_paths[j].parent.name\n",
    "                path_key = f\"{path1} <|> {path2}\"\n",
    "                is_close[path_key] = torch.allclose(embeddings_sets[i], embeddings_sets[j])\n",
    "        is_close = {x : y for x, y in is_close.items() if y}\n",
    "        # raise NotImplementedError() # XXX\n",
    "        assert len(is_close) == 0, f\"Got {len(is_close)} embeddings (total n_embeddings={len(embeddings_sets)}) that are the same:\\n\\n{json.dumps(is_close, indent=2)}\"\n",
    "        return [\n",
    "            EmbeddingDataset(\n",
    "                embeddings=embeddings_set,\n",
    "                ids=ids_set,\n",
    "                documents=documents_set\n",
    "            )\n",
    "            for embeddings_set, ids_set, documents_set in zip(embeddings_sets, ids_sets, documents_sets)\n",
    "        ]\n",
    "\n",
    "loader = EmbeddingLoader(root_path=ARGUANA_PATH)\n",
    "embeddings_list: List[EmbeddingDataset] = loader.fetch_embeddings(corpus_folder=\"corpus\", model_folders=None) # get all the document/corpus embeddings\n",
    "print(\"Embeddings shape is\", embeddings_list[0].embeddings.shape, \"=\", embeddings_list[1].embeddings.shape, \"= ...\") # axis 0 is dataset, axis 1 is embedding\n",
    "print(\"Embeddings devices:\", \"embeddings1.device\", embeddings_list[0].embeddings.device, \"embeddings2.device\", embeddings_list[1].embeddings.device, \"etc...\") # shoulds be cpu cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can try to train linear transforms between embeddings...\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from pydantic import BaseModel\n",
    "import wandb\n",
    "from typing import Optional\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, source_embeddings: torch.Tensor, target_embeddings: torch.Tensor):\n",
    "        assert source_embeddings.shape == target_embeddings.shape\n",
    "        self.source_embeddings = source_embeddings\n",
    "        self.target_embeddings = target_embeddings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_embeddings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_embeddings[idx], self.target_embeddings[idx]\n",
    "\n",
    "class LinearTransformTrainerArgs(BaseModel):\n",
    "    test_split: float = 0.2\n",
    "    num_epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    save_every_n_epochs: int = 10\n",
    "    use_tqdm: bool = True\n",
    "\n",
    "class LinearTransformTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        save_path: Path,\n",
    "        linear: Optional[nn.Linear],\n",
    "        source_embeddings: torch.Tensor,\n",
    "        target_embeddings: torch.Tensor,\n",
    "        device: torch.device | str,\n",
    "        args: LinearTransformTrainerArgs = LinearTransformTrainerArgs()\n",
    "    ):\n",
    "        self.linear = linear\n",
    "        self.source_embeddings = source_embeddings\n",
    "        self.target_embeddings = target_embeddings\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.device = device\n",
    "        self.test_split = args.test_split\n",
    "        self.save_every_n_epochs = args.save_every_n_epochs\n",
    "        self.save_path = save_path\n",
    "        self.checkpoint_path = save_path / \"checkpoints\"\n",
    "        self.checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.logfile = save_path / \"log.jsonl\"\n",
    "        self.use_tqdm = args.use_tqdm\n",
    "        if self.linear is None:\n",
    "            self.linear = self.create_linear_transform()\n",
    "        self.optimizer = torch.optim.Adam(self.linear.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def create_datasets(self):\n",
    "        # Create indices and shuffle\n",
    "        num_samples = len(self.source_embeddings)\n",
    "        indices = torch.randperm(num_samples)\n",
    "        \n",
    "        # Split indices\n",
    "        split_idx = int(num_samples * (1 - self.test_split))\n",
    "        train_indices = indices[:split_idx]\n",
    "        test_indices = indices[split_idx:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = EmbeddingDataset(\n",
    "            self.source_embeddings[train_indices],\n",
    "            self.target_embeddings[train_indices]\n",
    "        )\n",
    "        test_dataset = EmbeddingDataset(\n",
    "            self.source_embeddings[test_indices],\n",
    "            self.target_embeddings[test_indices]\n",
    "        )\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def create_linear_transform(self):\n",
    "        return nn.Linear(self.source_embeddings.shape[1], self.target_embeddings.shape[1]).to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_dataset, test_dataset = self.create_datasets()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        mse_loss = nn.MSELoss()\n",
    "        trange = tqdm.trange if self.use_tqdm else range\n",
    "        tqdm_fn = tqdm.tqdm if self.use_tqdm else lambda *args, **kwargs: args[0]\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            # Training\n",
    "            self.linear.train()\n",
    "            train_mse = 0.0\n",
    "            train_mae = 0.0\n",
    "            num_train_batches = 0\n",
    "\n",
    "            for source_emb, target_emb in train_loader:\n",
    "                source_emb = source_emb.to(self.device)\n",
    "                target_emb = target_emb.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.linear(source_emb)\n",
    "\n",
    "                loss = mse_loss(output, target_emb)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_mse += loss.detach().item()\n",
    "                train_mae += (output.detach() - target_emb.detach()).abs().mean().item()\n",
    "                num_train_batches += 1\n",
    "\n",
    "            avg_train_mse = train_mse / num_train_batches\n",
    "            avg_train_mae = train_mae / num_train_batches\n",
    "\n",
    "            # Evaluation\n",
    "            self.linear.eval()\n",
    "            test_mse = 0.0\n",
    "            test_mae = 0.0\n",
    "            num_test_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for source_emb, target_emb in tqdm_fn(test_loader):\n",
    "                    source_emb = source_emb.to(self.device)\n",
    "                    target_emb = target_emb.to(self.device)\n",
    "\n",
    "                    output = self.linear(source_emb)\n",
    "\n",
    "                    test_mse += mse_loss(output, target_emb).item()\n",
    "                    test_mae += (output.detach() - target_emb.detach()).abs().mean().item()\n",
    "                    num_test_batches += 1\n",
    "\n",
    "            avg_test_mse = test_mse / num_test_batches\n",
    "            avg_test_mae = test_mae / num_test_batches\n",
    "\n",
    "            # Log metrics\n",
    "            log_entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_mse\": avg_train_mse,\n",
    "                \"train_mae\": avg_train_mae,\n",
    "                \"test_mse\": avg_test_mse,\n",
    "                \"test_mae\": avg_test_mae,\n",
    "            }\n",
    "            wandb.log(log_entry)\n",
    "            with open(self.logfile, \"a\") as f:\n",
    "                f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "            if epoch % self.save_every_n_epochs == 0:\n",
    "                self.save_checkpoint(epoch)\n",
    "\n",
    "    def save_checkpoint(self, epoch: int):\n",
    "        checkpoint_path = self.checkpoint_path / f\"checkpoint_{epoch}.safetensors\"\n",
    "        safetensors.torch.save_file(self.linear.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ARGS = LinearTransformTrainerArgs(\n",
    "    test_split=0.2,\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    save_every_n_epochs=10,\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 110.93it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.14it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.70it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.36it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 140.78it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.31it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 108.90it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 104.31it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 104.40it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 101.83it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 116.57it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 106.01it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.30it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 145.66it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.16it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 114.30it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.36it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.53it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 151.76it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 107.61it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 240.73it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 143.80it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 130.98it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 206.47it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 124.29it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 144.10it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 268.80it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.74it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 110.00it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 249.19it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.29it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.98it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.29it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 120.10it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.53it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 117.98it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.95it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 101.56it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.95it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.74it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.65it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 111.18it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 104.02it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 115.38it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 107.89it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.28it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.66it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 100.33it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 106.62it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 101.75it/s]\n",
      "100%|██████████| 50/50 [01:34<00:00,  1.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>test_mae</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mse</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mse</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>test_mae</td><td>0.00084</td></tr><tr><td>test_mse</td><td>0.0</td></tr><tr><td>train_mae</td><td>0.00077</td></tr><tr><td>train_mse</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ipynb_test</strong> at: <a href='https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train/runs/seanydl5' target=\"_blank\">https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train/runs/seanydl5</a><br/> View project at: <a href='https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train' target=\"_blank\">https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241209_164747-seanydl5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: this shit is rlly fast!\n",
    "import click\n",
    "import os\n",
    "import shutil\n",
    "save_path_parent = Path(\"/mnt/align3_drive/adrianoh/git/dl_final_project_layers\")\n",
    "save_path = save_path_parent / \"ipynb_test\"\n",
    "assert os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None\n",
    "if save_path.exists():\n",
    "    click.echo(f\"Save path {save_path} already exists. Deleting it...\")\n",
    "    # click.confirm(f\"Save path {save_path} already exists. Do you want to delete it?\", abort=True)\n",
    "    shutil.rmtree(save_path)\n",
    "device = \"cuda:0\" # change this based on availability\n",
    "embeddings1 = embeddings1.to(device)\n",
    "embeddings2 = embeddings2.to(device)\n",
    "wandb.init(project=\"2024_12_09_dl_project_testing_layer_train\", name=\"ipynb_test\")\n",
    "trainer = LinearTransformTrainer(\n",
    "    save_path=save_path, # made by the trainer\n",
    "    linear=None, # trainer makes it\n",
    "    source_embeddings=embeddings1,\n",
    "    target_embeddings=embeddings2,\n",
    "    device=device,\n",
    "    args=DEFAULT_ARGS\n",
    ")\n",
    "trainer.train()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMetadata(pydantic.BaseModel):\n",
    "    \"\"\"ChromaDB embedding metadata.\n",
    "\n",
    "    Every embedding in a ChromaDB collection has a metadata object that explains\n",
    "    basically whether its a query or a document.\n",
    "    \"\"\"\n",
    "\n",
    "    record_id: str\n",
    "    chunk_id: str\n",
    "    chunk_text: str\n",
    "    record_text: str | None = None  # <---- like never used tbh\n",
    "    record_type: Literal[\"query\", \"document\"]\n",
    "    # TODO(Adriano) not everything should be train by default\n",
    "    record_split: Literal[\"train\", \"test\"] = \"train\"\n",
    "    tags: dict[str, str] | None = {}  # <---- should insert some meaning tags for umap cluster\n",
    "\n",
    "class IngestionSettings(BaseModel):\n",
    "    \"\"\"Equivalent to owler's .env file.\n",
    "\n",
    "    Used to define how to process the ingestion of the textual data into chromaDB\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_size: int = 256\n",
    "    device: str | None = None  # does not matter\n",
    "    distance_function: str | None = None  # does not matter\n",
    "    normalize_embeddings: bool | None = None  # does not matter\n",
    "    # TODO(Adriano) in the future we will want to try passing this through a model before\n",
    "    chunk_preprocessing_mode: Literal[\"add_prefix\"] = \"add_prefix\"\n",
    "    query_preprocessing_mode: Literal[\"add_prefix\"] = \"add_prefix\"\n",
    "    chunk_prefix: str = (\n",
    "        \"passage: \"  # Can be used to add prefix to text embeddings stored in vector store\n",
    "    )\n",
    "    query_prefix: str = (\n",
    "        \"query: \"  # Can be used to add prefix to text embeddings used for semantic search\n",
    "    )\n",
    "    chunk_overlap: int = 25  # Determines, for a given chunk of text, how many tokens must overlap with adjacent chunks.\n",
    "    dataloader_batch_size: int = 32\n",
    "    dataloader_num_workers: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings_list\u001b[49m)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dest \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embeddings_list)):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m src \u001b[38;5;241m!=\u001b[39m dest:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_list' is not defined"
     ]
    }
   ],
   "source": [
    "class ModelMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a single model's embeddings.\"\"\"\n",
    "    model: str\n",
    "    dataset: str = \"default\"\n",
    "    chunk_size: int = 256\n",
    "    query_prefix: str | None = None\n",
    "    document_prefix: str | None = None\n",
    "\n",
    "class TransformMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a transform between two models.\"\"\"\n",
    "    source_model: ModelMetadata\n",
    "    target_model: ModelMetadata\n",
    "\n",
    "class EmbeddingTransformTrainer:\n",
    "    \"\"\"Handles training transforms between embedding models.\"\"\"\n",
    "    \n",
    "    def __init__(self, save_path_parent: Path, device: str):\n",
    "        self.save_path_parent = save_path_parent\n",
    "        self.device = device\n",
    "        \n",
    "    def train_all_pairs(self, embeddings_list: list):\n",
    "        \"\"\"Train transforms between all pairs of embeddings.\"\"\"\n",
    "        for src in range(len(embeddings_list)):\n",
    "            for dest in range(len(embeddings_list)):\n",
    "                if src != dest:\n",
    "                    self._train_pair(embeddings_list[src], embeddings_list[dest])\n",
    "                    \n",
    "    def _train_pair(self, src_embeddings, dest_embeddings):\n",
    "        \"\"\"Train transform between a single pair of embeddings.\"\"\"\n",
    "        src_name = src_embeddings.model_name.replace(\"/\", \"_\")\n",
    "        dest_name = dest_embeddings.model_name.replace(\"/\", \"_\")\n",
    "        save_path = self.save_path_parent / f\"{src_name}_{dest_name}\"\n",
    "        \n",
    "        # Create and save metadata\n",
    "        metadata = TransformMetadata(\n",
    "            source_model=ModelMetadata(model=src_embeddings.model_name),\n",
    "            target_model=ModelMetadata(model=dest_embeddings.model_name)\n",
    "        )\n",
    "        \n",
    "        if save_path.exists():\n",
    "            shutil.rmtree(save_path)\n",
    "        save_path.mkdir(parents=True)\n",
    "        \n",
    "        with open(save_path / \"metadata.json\", \"w\") as f:\n",
    "            json.dump(metadata.model_dump(), f, indent=2)\n",
    "        \n",
    "        # Initialize wandb run\n",
    "        wandb.init(\n",
    "            project=\"2024_12_09_dl_project_testing_layer_train\",\n",
    "            name=f\"{src_name}_to_{dest_name}\",\n",
    "            reinit=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer = LinearTransformTrainer(\n",
    "            save_path=save_path,\n",
    "            linear=None,\n",
    "            source_embeddings=src_embeddings.embeddings.to(self.device),\n",
    "            target_embeddings=dest_embeddings.embeddings.to(self.device),\n",
    "            device=self.device,\n",
    "            args=DEFAULT_ARGS\n",
    "        )\n",
    "        trainer.train()\n",
    "        \n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
