{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Got 120 embeddings (total n_embeddings=16) that are the same:\n\n{\n  \"intfloat_e5-small-v2 <|> intfloat_e5-large-v2\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-small-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-small-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-large-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-large-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-small\": true,\n  \"sentence-transformers_sentence-t5-large <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-large-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> intfloat_e5-base-v2\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-base-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-small <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"thenlper_gte-small <|> BAAI_bge-large-en-v1.5\": true,\n  \"thenlper_gte-small <|> intfloat_e5-base-v2\": true,\n  \"thenlper_gte-small <|> BAAI_bge-base-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-large\": true,\n  \"thenlper_gte-small <|> thenlper_gte-base\": true,\n  \"thenlper_gte-small <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-small <|> thenlper_gte-large\": true,\n  \"thenlper_gte-small <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-small <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-small <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-large-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> intfloat_e5-base-v2\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-base-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_sentence-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> WhereIsAI_UAE-Large-V1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-small-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> intfloat_e5-base-v2\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-base-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-base-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_gtr-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_gtr-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-base <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-base <|> thenlper_gte-large\": true,\n  \"thenlper_gte-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-base <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> BAAI_bge-small-en-v1.5\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> sentence-transformers_gtr-t5-base\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-small-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-small-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 95\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     86\u001b[0m             EmbeddingDataset(\n\u001b[1;32m     87\u001b[0m                 embeddings\u001b[38;5;241m=\u001b[39membeddings_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m embeddings_set, ids_set, documents_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embeddings_sets, ids_sets, documents_sets)\n\u001b[1;32m     92\u001b[0m         ]\n\u001b[1;32m     94\u001b[0m loader \u001b[38;5;241m=\u001b[39m EmbeddingLoader(root_path\u001b[38;5;241m=\u001b[39mNFCORPUS_PATH)\n\u001b[0;32m---> 95\u001b[0m embeddings_list: List[EmbeddingDataset] \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_folders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get all the document/corpus embeddings\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings shape is\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m= ...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# axis 0 is dataset, axis 1 is embedding\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings devices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings1.device\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings2.device\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings_list[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metc...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# shoulds be cpu cpu\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 84\u001b[0m, in \u001b[0;36mEmbeddingLoader.fetch_embeddings\u001b[0;34m(self, corpus_folder, model_folders)\u001b[0m\n\u001b[1;32m     82\u001b[0m         is_close[path_key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(embeddings_sets[i], embeddings_sets[j])\n\u001b[1;32m     83\u001b[0m is_close \u001b[38;5;241m=\u001b[39m {x : y \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m is_close\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m y}\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(is_close) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(is_close)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings (total n_embeddings=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings_sets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that are the same:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(is_close,\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     86\u001b[0m     EmbeddingDataset(\n\u001b[1;32m     87\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings_set,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m embeddings_set, ids_set, documents_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embeddings_sets, ids_sets, documents_sets)\n\u001b[1;32m     92\u001b[0m ]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Got 120 embeddings (total n_embeddings=16) that are the same:\n\n{\n  \"intfloat_e5-small-v2 <|> intfloat_e5-large-v2\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-small-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-small-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-small-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-small-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-small-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-small\": true,\n  \"intfloat_e5-large-v2 <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-large-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> intfloat_e5-base-v2\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-large-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-large-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-large-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-large-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-small\": true,\n  \"sentence-transformers_sentence-t5-large <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-large-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> intfloat_e5-base-v2\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-base-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-small <|> Salesforce_SFR-Embedding-Mistral\": true,\n  \"thenlper_gte-small <|> BAAI_bge-large-en-v1.5\": true,\n  \"thenlper_gte-small <|> intfloat_e5-base-v2\": true,\n  \"thenlper_gte-small <|> BAAI_bge-base-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-large\": true,\n  \"thenlper_gte-small <|> thenlper_gte-base\": true,\n  \"thenlper_gte-small <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-small <|> thenlper_gte-large\": true,\n  \"thenlper_gte-small <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-small <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-small <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-small <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-large-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> intfloat_e5-base-v2\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-base-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_sentence-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> thenlper_gte-large\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> WhereIsAI_UAE-Large-V1\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> BAAI_bge-small-en-v1.5\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> sentence-transformers_gtr-t5-base\": true,\n  \"Salesforce_SFR-Embedding-Mistral <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> intfloat_e5-base-v2\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-base-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-large-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-large-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-large-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-large-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-base-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-large\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-base\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_sentence-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> thenlper_gte-large\": true,\n  \"intfloat_e5-base-v2 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"intfloat_e5-base-v2 <|> BAAI_bge-small-en-v1.5\": true,\n  \"intfloat_e5-base-v2 <|> sentence-transformers_gtr-t5-base\": true,\n  \"intfloat_e5-base-v2 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_sentence-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> thenlper_gte-large\": true,\n  \"BAAI_bge-base-en-v1.5 <|> WhereIsAI_UAE-Large-V1\": true,\n  \"BAAI_bge-base-en-v1.5 <|> BAAI_bge-small-en-v1.5\": true,\n  \"BAAI_bge-base-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-base-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_sentence-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> thenlper_gte-large\": true,\n  \"sentence-transformers_gtr-t5-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_gtr-t5-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_gtr-t5-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_gtr-t5-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-base <|> sentence-transformers_sentence-t5-base\": true,\n  \"thenlper_gte-base <|> thenlper_gte-large\": true,\n  \"thenlper_gte-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_sentence-t5-base <|> thenlper_gte-large\": true,\n  \"sentence-transformers_sentence-t5-base <|> WhereIsAI_UAE-Large-V1\": true,\n  \"sentence-transformers_sentence-t5-base <|> BAAI_bge-small-en-v1.5\": true,\n  \"sentence-transformers_sentence-t5-base <|> sentence-transformers_gtr-t5-base\": true,\n  \"sentence-transformers_sentence-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"thenlper_gte-large <|> WhereIsAI_UAE-Large-V1\": true,\n  \"thenlper_gte-large <|> BAAI_bge-small-en-v1.5\": true,\n  \"thenlper_gte-large <|> sentence-transformers_gtr-t5-base\": true,\n  \"thenlper_gte-large <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> BAAI_bge-small-en-v1.5\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> sentence-transformers_gtr-t5-base\": true,\n  \"WhereIsAI_UAE-Large-V1 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"BAAI_bge-small-en-v1.5 <|> sentence-transformers_gtr-t5-base\": true,\n  \"BAAI_bge-small-en-v1.5 <|> mixedbread-ai_mxbai-embed-large-v1\": true,\n  \"sentence-transformers_gtr-t5-base <|> mixedbread-ai_mxbai-embed-large-v1\": true\n}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import safetensors\n",
    "import pydantic\n",
    "import safetensors.torch\n",
    "from typing import Literal, Optional, List, Dict, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "DATASETS_PATH = Path(\"/mnt/align3_drive/adrianoh/dl_final_project_embeddings\")\n",
    "NFCORPUS_PATH = DATASETS_PATH / \"nfcorpus-real-other-is-scidocs\"\n",
    "SCIDOCS_PATH = DATASETS_PATH / \"nfcorpus\"\n",
    "FIQA_PATH = DATASETS_PATH / \"fiqa\"\n",
    "ARGUANA_PATH = DATASETS_PATH / \"arguana\"\n",
    "\n",
    "class EmbeddingDataset(pydantic.BaseModel):\n",
    "    config: pydantic.ConfigDict = pydantic.ConfigDict(arbitrary_types_allowed=True)\n",
    "    embeddings: torch.Tensor\n",
    "    ids: List[str]\n",
    "    documents: List[str]\n",
    "\n",
    "class EmbeddingLoader:\n",
    "    def __init__(self, root_path: Path):\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def fetch_embeddings(\n",
    "        self,\n",
    "        corpus_folder: Literal[\"corpus\", \"query\"] = \"corpus\",\n",
    "        model_folders: Optional[List[str]] = None\n",
    "    ) -> List[EmbeddingDataset]:\n",
    "        \"\"\"\n",
    "        Fetch the set of embeddings for the given corpus and subfolders.\n",
    "        If it's `None` for `model_folders`, it will fetch all the models' embeddings.\n",
    "        \"\"\"\n",
    "        if model_folders is None:\n",
    "            model_folders = [model_folder.name for model_folder in (self.root_path / corpus_folder).iterdir() if model_folder.is_dir()] # fmt: skip\n",
    "        subpaths = [(self.root_path / corpus_folder / model_folder) for model_folder in model_folders]\n",
    "        assert len(set(subpath.name for subpath in subpaths)) == len(subpaths)\n",
    "        # The files we want\n",
    "        embeddings_paths = [subpath / \"embeddings.safetensors\" for subpath in subpaths]\n",
    "        ids_paths = [subpath / \"ids.jsonl\" for subpath in subpaths]\n",
    "        documents_paths = [subpath / \"documents.jsonl\" for subpath in subpaths]\n",
    "\n",
    "        # They should all have the correct files present\n",
    "        assert all(embeddings_path.exists() for embeddings_path in embeddings_paths), (\"Not all embeddings paths exist:\\n  \" + '\\n  '.join(f'{p.as_posix()}: {p.exists()}' for p in embeddings_paths)) # fmt: skip\n",
    "        assert all(ids_path.exists() for ids_path in ids_paths), f\"Not all ids paths exist: {ids_paths}\"\n",
    "        assert all(documents_path.exists() for documents_path in documents_paths), f\"Not all documents paths exist: {documents_paths}\" # fmt: skip\n",
    "\n",
    "        _embeddings_sets: List[Dict[str, torch.Tensor]] = [safetensors.torch.load_file(embeddings_path, device=\"cpu\") for embeddings_path in embeddings_paths] # fmt: skip\n",
    "        _ids_sets: List[Dict[str, List[str]]] = [json.load(open(ids_path, \"r\")) for ids_path in ids_paths]\n",
    "        _documents_sets: List[Dict[str, List[str]]] = [json.load(open(documents_path, \"r\")) for documents_path in documents_paths] # fmt: skip\n",
    "\n",
    "        # Extrat the jsons\n",
    "        assert all(isinstance(ids_set, dict) and \"ids\" in ids_set and len(ids_set) == 1 for ids_set in _ids_sets)\n",
    "        assert all(isinstance(documents_set, dict) and \"documents\" in documents_set and len(documents_set) == 1 for documents_set in _documents_sets) # fmt: skip\n",
    "        assert all(isinstance(embeddings_set, dict) and \"embeddings\" in embeddings_set and len(embeddings_set) == 1 for embeddings_set in _embeddings_sets) # fmt: skip\n",
    "        ids_sets: List[List[str]] = [ids_set[\"ids\"] for ids_set in _ids_sets]\n",
    "        documents_sets: List[List[str]] = [documents_set[\"documents\"] for documents_set in _documents_sets] # fmt: skip\n",
    "        embeddings_sets: List[torch.Tensor] = [embeddings_set[\"embeddings\"] for embeddings_set in _embeddings_sets] # fmt: skip\n",
    "\n",
    "        # Make sure that all these have the same lengths\n",
    "        _ids_lengths = [len(ids_set) for ids_set in ids_sets]\n",
    "        _documents_lengths = [len(documents_set) for documents_set in documents_sets] # fmt: skip\n",
    "        _embeddings_lengths = [len(embeddings_set) for embeddings_set in embeddings_sets] # fmt: skip\n",
    "        assert len(set(_ids_lengths)) == 1\n",
    "        assert len(set(_documents_lengths)) == 1\n",
    "        assert len(set(_embeddings_lengths)) == 1\n",
    "\n",
    "        # Make sure the types are OK => they come from the same dataset but different models embedded so they should ONLY differ in that\n",
    "        assert all(isinstance(ids_set, list) and all(isinstance(id, str) for id in ids_set) for ids_set in ids_sets) # fmt: skip\n",
    "        assert all(ids_set1 == ids_sets[0] for ids_set1 in ids_sets) # star pattern for equality\n",
    "        # ...\n",
    "        assert all(isinstance(documents_set, list) and all(isinstance(doc, str) for doc in documents_set) for documents_set in documents_sets) # fmt: skip\n",
    "        assert all(documents_set1 == documents_sets[0] for documents_set1 in documents_sets) # star pattern for equality\n",
    "        # ...\n",
    "        assert all(isinstance(embeddings_set, torch.Tensor) and embeddings_set.shape == embeddings_sets[0].shape for embeddings_set in embeddings_sets) # fmt: skip\n",
    "        # Cartesian product: all most be unique\n",
    "        # avoid double-counting and self-comparison\n",
    "        is_close: Dict[str, bool] = {}\n",
    "        for i in range(len(embeddings_sets)):\n",
    "            for j in range(i + 1, len(embeddings_sets)):\n",
    "                path1, path2 = embeddings_paths[i].parent.name, embeddings_paths[j].parent.name\n",
    "                path_key = f\"{path1} <|> {path2}\"\n",
    "                is_close[path_key] = torch.allclose(embeddings_sets[i], embeddings_sets[j])\n",
    "        is_close = {x : y for x, y in is_close.items() if y}\n",
    "        assert len(is_close) == 0, f\"Got {len(is_close)} embeddings (total n_embeddings={len(embeddings_sets)}) that are the same:\\n\\n{json.dumps(is_close, indent=2)}\"\n",
    "        return [\n",
    "            EmbeddingDataset(\n",
    "                embeddings=embeddings_set,\n",
    "                ids=ids_set,\n",
    "                documents=documents_set\n",
    "            )\n",
    "            for embeddings_set, ids_set, documents_set in zip(embeddings_sets, ids_sets, documents_sets)\n",
    "        ]\n",
    "\n",
    "loader = EmbeddingLoader(root_path=NFCORPUS_PATH)\n",
    "embeddings_list: List[EmbeddingDataset] = loader.fetch_embeddings(corpus_folder=\"corpus\", model_folders=None) # get all the document/corpus embeddings\n",
    "print(\"Embeddings shape is\", embeddings_list[0].embeddings.shape, \"=\", embeddings_list[1].embeddings.shape, \"= ...\") # axis 0 is dataset, axis 1 is embedding\n",
    "print(\"Embeddings devices:\", \"embeddings1.device\", embeddings_list[0].embeddings.device, \"embeddings2.device\", embeddings_list[1].embeddings.device, \"etc...\") # shoulds be cpu cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can try to train linear transforms between embeddings...\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from pydantic import BaseModel\n",
    "import wandb\n",
    "from typing import Optional\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, source_embeddings: torch.Tensor, target_embeddings: torch.Tensor):\n",
    "        assert source_embeddings.shape == target_embeddings.shape\n",
    "        self.source_embeddings = source_embeddings\n",
    "        self.target_embeddings = target_embeddings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_embeddings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_embeddings[idx], self.target_embeddings[idx]\n",
    "\n",
    "class LinearTransformTrainerArgs(BaseModel):\n",
    "    test_split: float = 0.2\n",
    "    num_epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    save_every_n_epochs: int = 10\n",
    "    use_tqdm: bool = True\n",
    "\n",
    "class LinearTransformTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        save_path: Path,\n",
    "        linear: Optional[nn.Linear],\n",
    "        source_embeddings: torch.Tensor,\n",
    "        target_embeddings: torch.Tensor,\n",
    "        device: torch.device | str,\n",
    "        args: LinearTransformTrainerArgs = LinearTransformTrainerArgs()\n",
    "    ):\n",
    "        self.linear = linear\n",
    "        self.source_embeddings = source_embeddings\n",
    "        self.target_embeddings = target_embeddings\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.device = device\n",
    "        self.test_split = args.test_split\n",
    "        self.save_every_n_epochs = args.save_every_n_epochs\n",
    "        self.save_path = save_path\n",
    "        self.checkpoint_path = save_path / \"checkpoints\"\n",
    "        self.checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.logfile = save_path / \"log.jsonl\"\n",
    "        self.use_tqdm = args.use_tqdm\n",
    "        if self.linear is None:\n",
    "            self.linear = self.create_linear_transform()\n",
    "        self.optimizer = torch.optim.Adam(self.linear.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def create_datasets(self):\n",
    "        # Create indices and shuffle\n",
    "        num_samples = len(self.source_embeddings)\n",
    "        indices = torch.randperm(num_samples)\n",
    "        \n",
    "        # Split indices\n",
    "        split_idx = int(num_samples * (1 - self.test_split))\n",
    "        train_indices = indices[:split_idx]\n",
    "        test_indices = indices[split_idx:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = EmbeddingDataset(\n",
    "            self.source_embeddings[train_indices],\n",
    "            self.target_embeddings[train_indices]\n",
    "        )\n",
    "        test_dataset = EmbeddingDataset(\n",
    "            self.source_embeddings[test_indices],\n",
    "            self.target_embeddings[test_indices]\n",
    "        )\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def create_linear_transform(self):\n",
    "        return nn.Linear(self.source_embeddings.shape[1], self.target_embeddings.shape[1]).to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_dataset, test_dataset = self.create_datasets()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        mse_loss = nn.MSELoss()\n",
    "        trange = tqdm.trange if self.use_tqdm else range\n",
    "        tqdm_fn = tqdm.tqdm if self.use_tqdm else lambda *args, **kwargs: args[0]\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            # Training\n",
    "            self.linear.train()\n",
    "            train_mse = 0.0\n",
    "            train_mae = 0.0\n",
    "            num_train_batches = 0\n",
    "\n",
    "            for source_emb, target_emb in train_loader:\n",
    "                source_emb = source_emb.to(self.device)\n",
    "                target_emb = target_emb.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.linear(source_emb)\n",
    "\n",
    "                loss = mse_loss(output, target_emb)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_mse += loss.detach().item()\n",
    "                train_mae += (output.detach() - target_emb.detach()).abs().mean().item()\n",
    "                num_train_batches += 1\n",
    "\n",
    "            avg_train_mse = train_mse / num_train_batches\n",
    "            avg_train_mae = train_mae / num_train_batches\n",
    "\n",
    "            # Evaluation\n",
    "            self.linear.eval()\n",
    "            test_mse = 0.0\n",
    "            test_mae = 0.0\n",
    "            num_test_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for source_emb, target_emb in tqdm_fn(test_loader):\n",
    "                    source_emb = source_emb.to(self.device)\n",
    "                    target_emb = target_emb.to(self.device)\n",
    "\n",
    "                    output = self.linear(source_emb)\n",
    "\n",
    "                    test_mse += mse_loss(output, target_emb).item()\n",
    "                    test_mae += (output.detach() - target_emb.detach()).abs().mean().item()\n",
    "                    num_test_batches += 1\n",
    "\n",
    "            avg_test_mse = test_mse / num_test_batches\n",
    "            avg_test_mae = test_mae / num_test_batches\n",
    "\n",
    "            # Log metrics\n",
    "            log_entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_mse\": avg_train_mse,\n",
    "                \"train_mae\": avg_train_mae,\n",
    "                \"test_mse\": avg_test_mse,\n",
    "                \"test_mae\": avg_test_mae,\n",
    "            }\n",
    "            wandb.log(log_entry)\n",
    "            with open(self.logfile, \"a\") as f:\n",
    "                f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "            if epoch % self.save_every_n_epochs == 0:\n",
    "                self.save_checkpoint(epoch)\n",
    "\n",
    "    def save_checkpoint(self, epoch: int):\n",
    "        checkpoint_path = self.checkpoint_path / f\"checkpoint_{epoch}.safetensors\"\n",
    "        safetensors.torch.save_file(self.linear.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ARGS = LinearTransformTrainerArgs(\n",
    "    test_split=0.2,\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    save_every_n_epochs=10,\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 110.93it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.14it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.70it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.36it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 140.78it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.31it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 108.90it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 104.31it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 104.40it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 101.83it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 116.57it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 106.01it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.30it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 145.66it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.16it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 114.30it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.36it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.53it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 151.76it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 107.61it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 240.73it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 143.80it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 130.98it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 206.47it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 124.29it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 144.10it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 268.80it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.74it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 110.00it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 249.19it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.29it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.98it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.29it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 120.10it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.53it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 117.98it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.95it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 101.56it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.95it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 105.74it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 102.65it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 111.18it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 104.02it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 115.38it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 107.89it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.28it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 103.66it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 100.33it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 106.62it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 101.75it/s]\n",
      "100%|██████████| 50/50 [01:34<00:00,  1.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>test_mae</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mse</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mse</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>test_mae</td><td>0.00084</td></tr><tr><td>test_mse</td><td>0.0</td></tr><tr><td>train_mae</td><td>0.00077</td></tr><tr><td>train_mse</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ipynb_test</strong> at: <a href='https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train/runs/seanydl5' target=\"_blank\">https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train/runs/seanydl5</a><br/> View project at: <a href='https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train' target=\"_blank\">https://wandb.ai/4gate/2024_12_09_dl_project_testing_layer_train</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241209_164747-seanydl5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: this shit is rlly fast!\n",
    "import click\n",
    "import os\n",
    "import shutil\n",
    "save_path_parent = Path(\"/mnt/align3_drive/adrianoh/git/dl_final_project_layers\")\n",
    "save_path = save_path_parent / \"ipynb_test\"\n",
    "assert os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None\n",
    "if save_path.exists():\n",
    "    click.echo(f\"Save path {save_path} already exists. Deleting it...\")\n",
    "    # click.confirm(f\"Save path {save_path} already exists. Do you want to delete it?\", abort=True)\n",
    "    shutil.rmtree(save_path)\n",
    "device = \"cuda:0\" # change this based on availability\n",
    "embeddings1 = embeddings1.to(device)\n",
    "embeddings2 = embeddings2.to(device)\n",
    "wandb.init(project=\"2024_12_09_dl_project_testing_layer_train\", name=\"ipynb_test\")\n",
    "trainer = LinearTransformTrainer(\n",
    "    save_path=save_path, # made by the trainer\n",
    "    linear=None, # trainer makes it\n",
    "    source_embeddings=embeddings1,\n",
    "    target_embeddings=embeddings2,\n",
    "    device=device,\n",
    "    args=DEFAULT_ARGS\n",
    ")\n",
    "trainer.train()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
