{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This training script trains MLPs up to layer 5?\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "DATASETS = [\n",
    "    # (numbers are counts for documents, there may be some longer documents -> slightly more chunks)\n",
    "    \"arguana\",  # 10K\n",
    "    \"fiqa\",  # 50K -> 20K\n",
    "    \"scidocs\",  # 25K -> 20K\n",
    "    \"nfcorpus\",  # 5K\n",
    "    \"hotpotqa\",  # 100K -> 20K\n",
    "    \"trec-covid\",  # too much -> 20K\n",
    "]\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"WhereIsAI/UAE-Large-V1\",\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    \"intfloat/e5-base-v2\",\n",
    "    \"intfloat/e5-large-v2\",\n",
    "    \"intfloat/e5-small-v2\",\n",
    "    \"thenlper/gte-base\",\n",
    "    \"thenlper/gte-large\",\n",
    "    \"thenlper/gte-small\",\n",
    "    \"sentence-transformers/gtr-t5-base\",\n",
    "    \"sentence-transformers/gtr-t5-large\",\n",
    "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    \"sentence-transformers/sentence-t5-base\",\n",
    "    \"sentence-transformers/sentence-t5-large\",\n",
    "    \"openai/text-embedding-3-large\",\n",
    "    \"openai/text-embedding-3-small\",\n",
    "]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_dims: list[int] | None = None,\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_layers = num_layers or len(hidden_dims)\n",
    "        if num_layers is None:\n",
    "            raise ValueError(\"Either num_layers or hidden_dims must be provided\")\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [max(input_dim, output_dim)] * (num_layers - 1)\n",
    "\n",
    "        # Build layer dimensions including input and output\n",
    "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "\n",
    "        # Create sequential model with linear layers and ReLU activations\n",
    "        layers: list[nn.Module] = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n",
    "            if i < len(layer_dims) - 2:  # No ReLU after final layer\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding src shape torch.Size([24900, 1024])\n",
      "embedding dst shape torch.Size([24900, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple scaling test to see how many we can train at once without running out of memory.\n",
    "\"\"\"\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "device = \"cuda:3\"\n",
    "embeddings_path = Path(\"/mnt/align3_drive/adrianoh/dl_final_project_embeddings\")\n",
    "embeddings_path_src = embeddings_path / MODEL_NAMES[0].replace(\"/\", \"_\") / DATASETS[1]\n",
    "embeddings_path_dst = embeddings_path / MODEL_NAMES[1].replace(\"/\", \"_\") / DATASETS[1]\n",
    "\n",
    "\n",
    "# NOTE: copied from cereal.py\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, source_embeddings: torch.Tensor, target_embeddings: torch.Tensor\n",
    "    ):\n",
    "        assert source_embeddings.shape[0] == target_embeddings.shape[0]\n",
    "        self.source_embeddings = source_embeddings\n",
    "        self.target_embeddings = target_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_embeddings[idx], self.target_embeddings[idx]\n",
    "\n",
    "\n",
    "def get_embeddings_paths(embeddings_path: Path):\n",
    "    record_type = \"corpus\"\n",
    "    embeddings_train_path = (\n",
    "        embeddings_path / f\"embeddings_{record_type}_train.safetensors\"\n",
    "    )\n",
    "    embeddings_validation_path = (\n",
    "        embeddings_path / f\"embeddings_{record_type}_validation.safetensors\"\n",
    "    )\n",
    "    assert (embeddings_train_path.exists() and embeddings_validation_path.exists()) or (\n",
    "        not embeddings_train_path.exists() and not embeddings_validation_path.exists()\n",
    "    )\n",
    "    if not embeddings_train_path.exists():\n",
    "        # NOTE: that sometimes the path names are reversed, i.e. when using OpenAI models; you can observe\n",
    "        # more in detail in `get_reversed_model_files` in `sanity_check_embeddings_note_equal.ipynb`\n",
    "        embeddings_train_path = (\n",
    "            embeddings_path / f\"{record_type}_train_embeddings.safetensors\"\n",
    "        )\n",
    "        embeddings_validation_path = (\n",
    "            embeddings_path / f\"{record_type}_validation_embeddings.safetensors\"\n",
    "        )\n",
    "    assert embeddings_train_path.exists() and embeddings_validation_path.exists(), f\"Files {embeddings_train_path} and {embeddings_validation_path} do not exist\"  # fmt: skip\n",
    "    return embeddings_train_path, embeddings_validation_path\n",
    "\n",
    "\n",
    "embeddings_train_path_src, embeddings_validation_path_src = get_embeddings_paths(\n",
    "    embeddings_path_src\n",
    ")\n",
    "embeddings_train_path_dst, embeddings_validation_path_dst = get_embeddings_paths(\n",
    "    embeddings_path_dst\n",
    ")\n",
    "# Load corpus validation embeddings\n",
    "embeddings_src = load_file(embeddings_train_path_src)[\"embeddings\"].to(device)\n",
    "embeddings_dst = load_file(embeddings_train_path_dst)[\"embeddings\"].to(device)\n",
    "print(\"embedding src shape\", embeddings_src.shape)\n",
    "print(\"embedding dst shape\", embeddings_dst.shape)\n",
    "num_layers = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "linears_src2dst = [\n",
    "    MLP(embeddings_src.shape[1], embeddings_dst.shape[1], num_layers=n).to(device)\n",
    "    for n in num_layers\n",
    "]\n",
    "linears_dst2src = [\n",
    "    MLP(embeddings_dst.shape[1], embeddings_src.shape[1], num_layers=n).to(device)\n",
    "    for n in num_layers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 24900\n",
      "Time taken: 10.35 seconds\n",
      "Time per model epoch: 0.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Doing the scaling test still.\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "train_dataset = EmbeddingDataset(embeddings_src, embeddings_dst)\n",
    "loss_fn = nn.MSELoss()\n",
    "# NOTE\n",
    "# We will get time per epoch per layer\n",
    "# TOTAL_TIME = NUM_EPOCHS * NUM_MODELS * TIME_PER_MODEL_EPOCH\n",
    "num_epochs = 10\n",
    "batch_sizes = [len(train_dataset)]  # [32, 64, 128, 512, 1024, 4096, len(train_dataset)]\n",
    "for batch_size in batch_sizes:\n",
    "    #### START ####\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    all_parameters = []\n",
    "    for linears in linears_src2dst + linears_dst2src:\n",
    "        all_parameters.extend(list(linears.parameters()))\n",
    "    optimizer = torch.optim.Adam(all_parameters, lr=1e-3)\n",
    "    for linears in linears_src2dst + linears_dst2src:\n",
    "        linears.train()\n",
    "    for epoch in tqdm.trange(num_epochs):\n",
    "        for src_emb, dst_emb in train_loader:\n",
    "            for linear_src2dst, linear_dst2src in zip(\n",
    "                linears_src2dst, linears_dst2src, strict=False\n",
    "            ):\n",
    "                linear_src2dst.zero_grad()\n",
    "                linear_dst2src.zero_grad()\n",
    "                # one goes backwards, one forwards\n",
    "                output_src2dst = linear_src2dst(src_emb)\n",
    "                output_dst2src = linear_dst2src(dst_emb)\n",
    "                loss_src2dst = loss_fn(output_src2dst, dst_emb)\n",
    "                loss_dst2src = loss_fn(output_dst2src, src_emb)\n",
    "                loss = loss_src2dst + loss_dst2src\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    time_end = time.time()\n",
    "    time_per_model_epoch = (\n",
    "        (time_end - time_start) / num_epochs / len(linears_src2dst + linears_dst2src)\n",
    "    )\n",
    "    print(\"Batch size\", batch_size)\n",
    "    print(f\"Time taken: {time_end - time_start:.2f} seconds\")\n",
    "    print(f\"Time per model epoch: {time_per_model_epoch:.2f} seconds\")\n",
    "    #### END ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.14it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.27it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.45it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.44it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.35it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.37it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.32it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.34it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 24900\n",
      "Time taken: 27.23 seconds\n",
      "Time per model epoch: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Try flipping the order of the for loops.\n",
    "\"\"\"\n",
    "num_epochs = 10\n",
    "batch_sizes = [len(train_dataset)]  # [32, 64, 128, 512, 1024, 4096, len(train_dataset)]\n",
    "for batch_size in batch_sizes:\n",
    "    #### START ####\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    all_parameters = []\n",
    "    for linears in linears_src2dst + linears_dst2src:\n",
    "        all_parameters.extend(list(linears.parameters()))\n",
    "    optimizer = torch.optim.Adam(all_parameters, lr=1e-3)\n",
    "    for linears in linears_src2dst + linears_dst2src:\n",
    "        linears.train()\n",
    "    for linear_src2dst, linear_dst2src in zip(\n",
    "        linears_src2dst, linears_dst2src, strict=False\n",
    "    ):\n",
    "        for epoch in tqdm.trange(num_epochs):\n",
    "            for src_emb, dst_emb in train_loader:\n",
    "                linear_src2dst.zero_grad()\n",
    "                linear_dst2src.zero_grad()\n",
    "                # one goes backwards, one forwards\n",
    "                output_src2dst = linear_src2dst(src_emb)\n",
    "                output_dst2src = linear_dst2src(dst_emb)\n",
    "                loss_src2dst = loss_fn(output_src2dst, dst_emb)\n",
    "                loss_dst2src = loss_fn(output_dst2src, src_emb)\n",
    "                loss = loss_src2dst + loss_dst2src\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    time_end = time.time()\n",
    "    time_per_model_epoch = (\n",
    "        (time_end - time_start) / num_epochs / len(linears_src2dst + linears_dst2src)\n",
    "    )\n",
    "    print(\"Batch size\", batch_size)\n",
    "    print(f\"Time taken: {time_end - time_start:.2f} seconds\")\n",
    "    print(f\"Time per model epoch: {time_per_model_epoch:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 24900: 6.52 seconds\n",
      "Time per model per epoch: 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Claude thinks this will be faster.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_models(\n",
    "    src_emb: torch.Tensor,\n",
    "    dst_emb: torch.Tensor,\n",
    "    models_src2dst: list[MLP],\n",
    "    models_dst2src: list[MLP],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "):\n",
    "    batch_size = src_emb.shape[0]\n",
    "    num_models = len(models_src2dst)\n",
    "\n",
    "    # Expand embeddings to match number of models\n",
    "    src_emb_expanded = src_emb.unsqueeze(0).expand(\n",
    "        num_models, -1, -1\n",
    "    )  # [num_models, batch_size, dim]\n",
    "    dst_emb_expanded = dst_emb.unsqueeze(0).expand(num_models, -1, -1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass (all models at once)\n",
    "    outputs_src2dst = torch.stack([model(src_emb) for model in models_src2dst])\n",
    "    outputs_dst2src = torch.stack([model(dst_emb) for model in models_dst2src])\n",
    "\n",
    "    # Compute loss for all models simultaneously\n",
    "    loss_src2dst = loss_fn(outputs_src2dst, dst_emb_expanded)\n",
    "    loss_dst2src = loss_fn(outputs_dst2src, src_emb_expanded)\n",
    "    loss = loss_src2dst + loss_dst2src\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.mean().item()\n",
    "\n",
    "\n",
    "def training_run(\n",
    "    # NOTE: each element of the list gets the same dataset\n",
    "    # (i.e. these should correspond to different embeddings)\n",
    "    datasets: list[EmbeddingDataset],\n",
    "    models_src2dst: list[list[MLP]],\n",
    "    models_dst2src: list[list[MLP]],\n",
    "    loss_fn: nn.Module,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    ") -> float:\n",
    "    assert (\n",
    "        len(set(len(d) for d in datasets)) == 1\n",
    "    ), \"All datasets must have the same length\"\n",
    "    time_start = time.time()\n",
    "    train_loaders = [\n",
    "        DataLoader(d, batch_size=batch_size, shuffle=True) for d in datasets\n",
    "    ]\n",
    "    num_iters = len(train_loaders[0])  # NOTE: same for all of the datasets\n",
    "    all_parameters = []\n",
    "    # 1. Get all parameters\n",
    "    for models_list in models_src2dst + models_dst2src:\n",
    "        for models in models_list:\n",
    "            # (TODO Adriano not sure if joining this way is OK?)\n",
    "            all_parameters.extend(list(models.parameters()))\n",
    "    optimizer = torch.optim.Adam(all_parameters, lr=1e-3)\n",
    "    # 2. Set all models to train\n",
    "    for models_list in models_src2dst + models_dst2src:\n",
    "        for models in models_list:\n",
    "            models.train()\n",
    "    # 3. Train\n",
    "    for epoch in tqdm.trange(num_epochs):\n",
    "        loader_iters = [iter(loader) for loader in train_loaders]\n",
    "        for i in range(num_iters):\n",
    "            xys = [next(loader_iter) for loader_iter in loader_iters]\n",
    "            assert not any(\n",
    "                x is None for x in xys\n",
    "            ), \"Some loader iterators returned None\"\n",
    "            for (X, Y), models_src2dst_list, models_dst2src_list in zip(\n",
    "                xys, models_src2dst, models_dst2src, strict=False\n",
    "            ):\n",
    "                train_models(\n",
    "                    X, Y, models_src2dst_list, models_dst2src_list, optimizer, loss_fn\n",
    "                )\n",
    "    return time.time() - time_start\n",
    "\n",
    "\n",
    "train_dataset = EmbeddingDataset(embeddings_src, embeddings_dst)\n",
    "num_epochs = 10\n",
    "for batch_size in batch_sizes:\n",
    "    loss_fn = nn.MSELoss()\n",
    "    time_taken = training_run(\n",
    "        [train_dataset],\n",
    "        [linears_src2dst],\n",
    "        [linears_dst2src],\n",
    "        loss_fn,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "    )\n",
    "    print(f\"Batch size {batch_size}: {time_taken:.2f} seconds\")\n",
    "    print(\n",
    "        f\"Time per model per epoch: {time_taken / num_epochs / len(linears_src2dst + linears_dst2src):.2f} seconds\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing block 0/3 idx=0 of 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 634664.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:12<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:38<00:00, 38.23s/it]\n",
      " 33%|███▎      | 1/3 [00:51<01:42, 51.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 512: 38.26 seconds\n",
      "Time per layer per epoch: 2.13 seconds\n",
      "clearing gpu memory\n",
      "Processing block 1/3 idx=46 of 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 810663.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:14<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.59s/it]\n",
      " 67%|██████▋   | 2/3 [01:43<00:51, 51.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 512: 37.62 seconds\n",
      "Time per layer per epoch: 2.09 seconds\n",
      "clearing gpu memory\n",
      "Processing block 2/3 idx=92 of 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 753262.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:13<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.87s/it]\n",
      "100%|██████████| 3/3 [02:35<00:00, 51.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 512: 37.89 seconds\n",
      "Time per layer per epoch: 2.11 seconds\n",
      "clearing gpu memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:35<00:00, 51.78s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell is meant to try and get a reasonable estimate for the time and memory\n",
    "possible when trying to train a LOT in parallel (i.e. literally training \n",
    "all N^2 at the same time (if this is possible, it seems ideal?)\n",
    "\n",
    "NOTE: this is not optimally memory efficient because it loads every dataset N-1 times.\n",
    "\"\"\"\n",
    "# del linears_src2dst, linears_dst2src\n",
    "\n",
    "# free cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda:3\"\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "with torch.cuda.device(device):\n",
    "    torch.cuda.empty_cache()\n",
    "# ---> Now make a table of models for all N^2 combinations where the indexxing is\n",
    "# models[source][destination][layer_num] -> return None if src == destination else list of models\n",
    "unordered_pairs_all = [\n",
    "    (MODEL_NAMES[i], MODEL_NAMES[j]) for j in range(len(MODEL_NAMES)) for i in range(j)\n",
    "]\n",
    "random.seed(55)\n",
    "random.shuffle(unordered_pairs_all)\n",
    "unordered_pairs_block_size: int = math.ceil(len(unordered_pairs_all) / 3)\n",
    "_is = list(range(0, len(unordered_pairs_all), unordered_pairs_block_size))\n",
    "model2embeddings = {}\n",
    "for model_name in MODEL_NAMES:\n",
    "    embeddings_path = Path(\"/mnt/align3_drive/adrianoh/dl_final_project_embeddings\")\n",
    "    embeddings_path_src = embeddings_path / model_name.replace(\"/\", \"_\") / DATASETS[1]\n",
    "    model2embeddings[model_name] = load_file(embeddings_train_path_src)[\n",
    "        \"embeddings\"\n",
    "    ].to(device)\n",
    "for _, i in enumerate(tqdm.tqdm(_is)):\n",
    "    print(f\"Processing block {_}/{len(_is)} idx={i} of {len(unordered_pairs_all)}\")\n",
    "    unordered_pairs = unordered_pairs_all[i : i + unordered_pairs_block_size]\n",
    "    embeddings_tensors_src: list[torch.Tensor] = []\n",
    "    embeddings_tensors_dst: list[torch.Tensor] = []\n",
    "    for src, dst in tqdm.tqdm(unordered_pairs):\n",
    "        # Append by ptr ideally?\n",
    "        embeddings_tensors_src.append(model2embeddings[src])\n",
    "        embeddings_tensors_dst.append(model2embeddings[dst])\n",
    "    assert len(embeddings_tensors_src) == len(unordered_pairs)\n",
    "    assert len(embeddings_tensors_dst) == len(unordered_pairs)\n",
    "    print(\"creating models\")\n",
    "    models_src2dst: list[list[MLP]] = []\n",
    "    models_dst2src: list[list[MLP]] = []\n",
    "    # NOTE: maybe we just do layers <= len = 7 instead of going all the way to 10 (this is already like 30% of the models' depths)\n",
    "    n_layers_list = list(range(2, 8))  # Maybe two blocks: [2,3], [4,5], [6,7] ???\n",
    "    # assert len(n_layers_list) <= 3\n",
    "    for (src, dst), src_emb, dst_emb in tqdm.tqdm(\n",
    "        list(\n",
    "            zip(\n",
    "                unordered_pairs,\n",
    "                embeddings_tensors_src,\n",
    "                embeddings_tensors_dst,\n",
    "                strict=False,\n",
    "            )\n",
    "        )\n",
    "    ):\n",
    "        src_dim, dst_dim = src_emb.shape[1], dst_emb.shape[1]\n",
    "        models_src2dst.append(\n",
    "            [MLP(src_dim, dst_dim, num_layers=n).to(device) for n in n_layers_list]\n",
    "        )\n",
    "        models_dst2src.append(\n",
    "            [MLP(dst_dim, src_dim, num_layers=n).to(device) for n in n_layers_list]\n",
    "        )\n",
    "    assert len(models_src2dst) == len(models_dst2src) == len(unordered_pairs)\n",
    "\n",
    "    print(\"training for 1 epoch\")\n",
    "    assert isinstance(models_src2dst, list)\n",
    "    assert isinstance(models_dst2src, list)\n",
    "    assert all(isinstance(models_src2dst[i], list) for i in range(len(models_src2dst)))\n",
    "    assert all(isinstance(models_dst2src[i], list) for i in range(len(models_dst2src)))\n",
    "    assert all(\n",
    "        all(\n",
    "            isinstance(models_src2dst[i][j], MLP) for j in range(len(models_src2dst[i]))\n",
    "        )\n",
    "        for i in range(len(models_src2dst))\n",
    "    )\n",
    "    assert all(\n",
    "        all(\n",
    "            isinstance(models_dst2src[i][j], MLP) for j in range(len(models_dst2src[i]))\n",
    "        )\n",
    "        for i in range(len(models_dst2src))\n",
    "    )\n",
    "    train_datasets = [\n",
    "        EmbeddingDataset(embeddings_src, embeddings_dst)\n",
    "        for embeddings_src, embeddings_dst in zip(\n",
    "            embeddings_tensors_src, embeddings_tensors_dst, strict=False\n",
    "        )\n",
    "    ]\n",
    "    assert len(train_datasets) == len(models_src2dst) == len(models_dst2src)\n",
    "    num_epochs = 1\n",
    "    # batch_sizes = [8192, 1024, 128]\n",
    "    batch_sizes = [512]  # I'm a little paranoid too bit will ruin results\n",
    "    for batch_size in batch_sizes:\n",
    "        loss_fn = nn.MSELoss()\n",
    "        time_taken = training_run(\n",
    "            train_datasets,\n",
    "            models_src2dst,\n",
    "            models_dst2src,\n",
    "            loss_fn,\n",
    "            num_epochs,\n",
    "            batch_size,\n",
    "        )\n",
    "        print(f\"Batch size {batch_size}: {time_taken:.2f} seconds\")\n",
    "        time_per_layer_per_epoch = (\n",
    "            time_taken / num_epochs / len(linears_src2dst + linears_dst2src)\n",
    "        )\n",
    "        print(f\"Time per layer per epoch: {time_per_layer_per_epoch:.2f} seconds\")\n",
    "    print(\"clearing gpu memory\")\n",
    "    del models_src2dst, models_dst2src, train_datasets\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conclusions from above:\n",
    "1. Larger batch size is actually a LOT faster? Not measured fully rigorously, but seems like it COULD be up to 20X faster.\n",
    "    Question: won't doing the entire dataset as a batch be bad?\n",
    "2. Running the dataloader in the outside seems better than\n",
    "    running it on the inside (i.e. all models \"more at once\")\n",
    "    is generally better. This gives maybe up to 2X?\n",
    "3. Batching loss is pretty good. Maybe up to 2X?\n",
    "4. It is possible to do all N^2 combinations but you run out of memory if you do > 1 model per combination\n",
    "5. It seems better to block by pairs more so than by models.\n",
    "\n",
    "Doing basically blocks of the N^2 combinations seems reasonable to me, doing all layers at once. I need to be careful to make sure\n",
    "I don't OOM though. The cause for the OOM seems to be that some of the pairs of models enforce some kind of bottleneck in memory at\n",
    "certain times and it's pretty bad.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
