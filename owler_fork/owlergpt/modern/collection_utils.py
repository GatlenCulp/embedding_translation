from __future__ import annotations
"""
Support utilities for various things pertaining to collections, parsing their names, etc...
"""
from typing import Tuple
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import os

OPENAI_MODELS = ["text-embedding-3-small", "text-embedding-3-large"]
COHERE_MODELS = ["embed-english-v3.0"]

MODEL_NAMES = [
    "Salesforce/SFR-Embedding-Mistral",
    "WhereIsAI/UAE-Large-V1",
    "BAAI/bge-base-en-v1.5",
    "BAAI/bge-large-en-v1.5",
    "BAAI/bge-small-en-v1.5",
    "intfloat/e5-base-v2",
    "intfloat/e5-large-v2",
    "intfloat/e5-small-v2",
    "thenlper/gte-base",
    "thenlper/gte-large",
    "thenlper/gte-small",
    "sentence-transformers/gtr-t5-base",
    "sentence-transformers/gtr-t5-large",
    "mixedbread-ai/mxbai-embed-large-v1",
    "sentence-transformers/sentence-t5-base",
    "sentence-transformers/sentence-t5-large",
    "openai/text-embedding-3-large",
    "openai/text-embedding-3-small",
]

def parse_collection_name(collection_name: str) -> Tuple[str, str, int]:
    """ Helper. Returns (dataset_name, model_name, chunk_size). """
    # example: scidocs_e5-base-v2_CharacterSplitting_256
    # NOTE that dataset name is selected_folder foldername
    # From `collection_name = f"{selected_folder}_{transformer_model}_CharacterSplitting_{tokens_per_chunk}"`
    # => <dataset_name>_<model_name>_CharacterSplitting_<chunk_size>
    dataset_name, model_name, _, chunk_size = collection_name.split("_")
    chunk_size = int(chunk_size)
    return dataset_name, model_name, chunk_size

class ModelLatentSizing:
    """
    Static class to support utilities for finding the sizes of models' latent dimensions
    (i.e. embedding dimensions).
    """
    @staticmethod
    def get_openai_model_size(model_name: str) -> int:
        """ Helper. """
        if model_name.startswith("openai/"):
            model_name = model_name[len("openai/"):]
        client = OpenAI(api_key=os.environ["OPENAI_KEY"]) # <---- export properly beforehand
        list_obj = client.embeddings.create(input=["hi"], model=model_name).data[0].embedding
        assert isinstance(list_obj, list)
        assert all(isinstance(x, float) for x in list_obj)
        return len(list_obj)

    @staticmethod
    def get_hf_model_size(model_name: str, device: str) -> int:
        """ Helper. """
        shape = SentenceTransformer(model_name, device=device).encode('hi').shape
        assert len(shape) == 1
        return shape[0]

# NOTE: this is the output (semi-autogenerated) of `ModelLatentSizing` functions
def model2model_dimension(model_name: str) -> int:
    """ Helper: get the size of the embedding dimension vector (1D, usually something like 768-4096). """
    # Miscellaneous (HF)
    if model_name == "SFR-Embedding-Mistral":
        return 4096
    elif model_name == "UAE-Large-V1":
        return 1024
    elif model_name == "mxbai-embed-large-v1":
        return 1024
    # BGE Models (HF)
    elif model_name == "bge-base-en-v1.5":
        return 768
    elif model_name == "bge-large-en-v1.5":
        return 1024
    elif model_name == "bge-small-en-v1.5":
        return 384
    #  E5 Models (HF)
    elif model_name == "e5-base-v2":
        return 768
    elif model_name == "e5-large-v2":
        return 1024
    elif model_name == "e5-small-v2":
        return 384
    # GTE Models (HF)
    elif model_name == "gte-base":
        return 768
    elif model_name == "gte-large":
        return 1024
    elif model_name == "gte-small":
        return 384
    # GTR-T5 Models (HF)
    elif model_name == "gtr-t5-base":
        return 768
    elif model_name == "gtr-t5-large":
        return 768
    # Sentence T5 (HF)
    elif model_name == "sentence-t5-base":
        return 768
    elif model_name == "sentence-t5-large":
        return 768
    # OpenAI Models
    elif model_name == "text-embedding-3-large":
        return 3072
    elif model_name == "text-embedding-3-small":
        return 1536
    else:
        # NOTE: cohere may be supported in THE FUTURE
        raise ValueError(f"Unsupported model: {model_name}")