from __future__ import annotations
"""
Generated by O1.
"""
import subprocess
import sys
from pathlib import Path

# Models (HF and openai) except Mistral
models = [
    "WhereIsAI/UAE-Large-V1",
    "BAAI/bge-base-en-v1.5",
    "BAAI/bge-large-en-v1.5",
    "BAAI/bge-small-en-v1.5",
    "intfloat/e5-base-v2",
    "intfloat/e5-large-v2",
    "intfloat/e5-small-v2",
    "thenlper/gte-base",
    "thenlper/gte-large",
    "thenlper/gte-small",
    "sentence-transformers/gtr-t5-base",
    "sentence-transformers/gtr-t5-large",
    "mixedbread-ai/mxbai-embed-large-v1",
    "sentence-transformers/sentence-t5-base",
    "sentence-transformers/sentence-t5-large",
    "openai/text-embedding-3-large",
    "openai/text-embedding-3-small",
]

# We exclude the Mistral model (Salesforce/SFR-Embedding-Mistral) if it were in the list, 
# but here it's not explicitly included in 'models' anyway. Just noting the requirement.
excluded_model_substring = "Mistral"
selected_models = [m for m in models if excluded_model_substring.lower() not in m.lower()]

# Directory containing datasets
chunks_dir = Path("./chunks")

# All datasets are simply all directories or jsonl files in chunks_dir. 
# The requirement: For *ALL* datasets in `./chunks/` directory
# Any `.jsonl` file inside a directory in `./chunks/` is considered a dataset component.
# We'll assume each subdirectory in `./chunks/` is a dataset, containing .jsonl files.
datasets = []
for item in chunks_dir.iterdir():
    if item.is_dir():
        # Check if directory has any `.jsonl` files
        if any(f.suffix == ".jsonl" for f in item.iterdir()):
            datasets.append(item.name)

# These parameters could be controlled here; you might want to iterate over multiple GPUs if you have them.
# For simplicity we just pick device0. Adjust as needed.
device_list = ["cuda:0"]  # you can have multiple devices if you want parallelization across GPUs
max_memory = "14GB"  # example: specify maximum cuda memory allowed

output_base_dir = "./embeddings_output"  # Where to store the output

# We'll spawn a subprocess per dataset per model. This can be large; be careful.
# The child script is expected to handle embedding and output generation.

def main():
    for dataset in datasets:
        for model_name in selected_models:
            for device_id, device in enumerate(device_list):
                cmd = [
                    sys.executable, "child.py",
                    "--dataset", dataset,
                    "--model", model_name,
                    "--device", device,
                    "--max-memory", max_memory,
                    "--chunks-dir", str(chunks_dir),
                    "--output-dir", output_base_dir
                ]
                print("Running:", " ".join(cmd))
                # Run and wait for it to finish. You could do asynchronous if you want parallelization.
                result = subprocess.run(cmd)
                if result.returncode != 0:
                    print(f"Error embedding dataset={dataset} with model={model_name} on device={device}. Return code: {result.returncode}")
                else:
                    print(f"Successfully embedded dataset={dataset} with model={model_name} on device={device}")


if __name__ == "__main__":
    main()
