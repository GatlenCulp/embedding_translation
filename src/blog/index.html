<!DOCTYPE html>
<meta charset="utf-8" />
<script src="https://distill.pub/template.v1.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
/>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"],
      ],
      displayMath: [
        ["$$", "$$"],
        ["\\[", "\\]"],
      ],
      processEscapes: true,
      processEnvironments: true,
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
    },
  };
</script>

<!-- https://distill.pub/guide/ -->
<!-- 
TODO: 
2. Fix citations
3. Add pretty graphics (maybe html/css animations created by claude)
 -->
<!-- 
 Examples
https://deep-learning-mit.github.io/staging/blog/ 

or 

https://distill.pub/2021/gnn-intro/

We can
-->

<script type="text/front-matter">
  title: "LEAD: Linear Embedding Alignment across Deep Neural Network Language Models' Representations"
  description: "Exploring whether different LLM-based semantic-search embedding models learn similar representations to push forward our understanding of LLM representations and lower the barrier to entry for semantic search and visualization."
  authors:
  - Gatlen Culp: https://gatlen.notion.site
  - Adriano Hernandez: https://linktr.ee/4gate
  affiliations:
  - MIT: http://web.mit.edu/
</script>

<dt-article class="centered">
  <div class="l-middle">
    <figure>
      <img
        src="./header.png"
        alt="Header visualization of semantic alignment across language models"
        style="width: 100%; max-height: 400px; object-fit: cover"
      />
    </figure>
  </div>
  <h1 id="title">
    LEAD: Linear Embedding Alignment across Deep Neural Network Language Models' Representations
  </h1>

  <div
    class="authors-section l-middle"
    style="text-align: center; margin-top: 2em"
  >
    <div class="authors" style="margin-bottom: 1em">
      <span style="font-size: 1.2em">
        <a
          href="https://gatlen.notion.site"
          style="text-decoration: none; color: #333"
          >Gatlen Culp</a
        >
        <span style="color: #666; margin: 0 0.5em">Â·</span>
        <a
          href="https://superurop.mit.edu/scholars/adriano-hernandez/"
          style="text-decoration: none; color: #333"
          >Adriano Hernandez</a
        >
      </span>
    </div>

    <div class="affiliations" style="color: #666; margin-bottom: 1em">
      Massachusetts Institute of Technology
    </div>

    <div class="date-info" style="color: #888; font-size: 0.9em">
      Published December 10th, 2024
    </div>
  </div>
  <!-- TODO: we MUST add some sort of control i.e. via random projections -->
  <h2 id="abstract">Abstract</h2>
  <p>
    Recent advances in Large Language Models (LLMs) have demonstrated their
    remarkable ability to capture and manipulate semantic information. In this
    work, we investigate a fundamental question: to what extent do different
    language embedding models learn similar semantic representations despite variations in
    architecture, training data, and initialization? Previous work briefly explored
    whether embedding models were similar by top-k result similarity as well as Centered
    Kernel Alignment (CKA), but found lukewarm results. After a reproduction of their work, <!-- not sure if we cite the subtle bug?-->
    we extend it to include a linear connectivity analysis: we train mappings between embedding spaces
    with linear layers. If the mappings are (approximately) bijective, then we consider those two spaces
    to be connectivity-aligned. Our findings over the span of 6 embedding datasets ranging from roughly
    5,000 to 20,000 documents and 18 models (ranging in size from 20-30 layers and including
    both open-source huggingface models and closed-source OpenAI models), showcases significant
    connectivity-alignment between models' semantic spaces in absolute terms, though there are a few outliers.
    Our findings have both theoretical and practical implications. Theoretically, they support
    the Platonic Representation Hypothesis and more weakly, the linear representation hypothesis.
    Practically, they have the potential to lower the cost of merging embedding datasets
    by a factor of at least 20. Our datasets, code, and results are publicly available in github and huggingface. <!--- might have to remove this? make sure to cite the bug-->
    
  </p>
  <!-- Honestly I don't know if this is such a great abstract. -->

  <h2 id="introduction">01 Background</h2>
  <!-- Think we maybe jumped into this a bit too soon
  <hr/> cool 
  -->

  <p>
    There is ample evidence supporting the idea that neural network representations may be
    aligned to some degree. Some notable observations include:
  </p>
  <!-- TODO: Add icons or something but this onslaught of bullets is a lot -->
    <ul style="list-style: none; padding-left: 0">
      <li
        style="
        margin-bottom: 1.5em;
        display: flex;
        gap: 1em;
        align-items: baseline;
      ">
      <i
        class="fas fa-chart-line"
        style="color: #666; width: 1.5em; text-align: center"
      ></i>
        <div>
          <b>Scaling Laws and Platonic Representations:</b> recent work
          has provided strong evidence that many different neural network models, even those trained on different datasets,
          or different sensory modalities, converge to a shared representation of the world <dt-cite key="huh_platonic_2024"></dt-cite>. Scaling
          laws based on quantized models of learning <dt-cite key="michaud2024quantizationmodelneuralscaling"></dt-cite>
           <dt-cite key="song2024resourcemodelneuralscaling"></dt-cite> also suggest at a compelling world-view in which
           different models, despite differing in architecture, training data, or training methodology, converge to shared
           representations of the world.
        </div>
    </li>
    <li
      style="
        margin-bottom: 1.5em;
        display: flex;
        gap: 1em;
        align-items: baseline;
      "
    >
      <i
        class="fas fa-network-wired"
        style="color: #666; width: 1.5em; text-align: center"
      ></i>
      <div>
        <b>Functionally similar components across neural networks:</b> For
        instance, induction heads have been observed in different language
        transformer architectures <dt-cite key="olsson2022context"></dt-cite>.
        Similar patterns have also been found in vision models
        <dt-cite key="cammarata_curve_2020"></dt-cite>
        <dt-cite key="schubert2021high-low"></dt-cite>
        <dt-cite key="olah2020zoom"></dt-cite>. We believe that such
        functionally similar components correspond to similar internal
        representations, and the language setting is relatively less explored.
      </div>
    </li>
    <li
      style="
        margin-bottom: 1.5em;
        display: flex;
        gap: 1em;
        align-items: baseline;
      "
    >
      <i
        class="fas fa-brain"
        style="color: #666; width: 1.5em; text-align: center"
      ></i>
      <div>
        <b>Consistent representations and behaviors:</b> Previous work has shown
        that different transformers exhibit consistent attention patterns for
        similar semantic concepts
        <dt-cite key="eberle-etal-2022-transformer"></dt-cite>, often aligning
        with human judgments. In vision, linear mappings can translate between
        representation spaces of different models, enabling interoperability
        <dt-cite key="hernandez_model_2023"></dt-cite>
        <dt-cite key="bansal_revisiting_2021"></dt-cite>. While there is debate
        and complexity in this field
        <dt-cite key="Klabunde2023TowardsMR"></dt-cite>, these findings suggest
        a common representational substrate across models.
      </div>
    </li>
    <li
      style="
        margin-bottom: 1.5em;
        display: flex;
        gap: 1em;
        align-items: baseline;
      "
    >
      <i
        class="fas fa-shield-halved"
        style="color: #666; width: 1.5em; text-align: center"
      ></i>
      <div>
        <b>Attack transferability:</b> Red-teaming and jailbreak prompts often
        transfer across different models, including black-box ones
        <dt-cite key="zou_universal_2023"></dt-cite>
        <dt-cite key="andriushchenko_jailbreaking_2024"></dt-cite>
        <dt-cite key="chao_jailbreaking_2024"></dt-cite>
        <dt-cite key="mehrotra_tree_2024"></dt-cite>. Such robustness of attacks
        may be due to underlying shared representational patterns, indicating
        that multiple models rely on similar internal structures.
      </div>
    </li>
    <li
      style="
        margin-bottom: 1.5em;
        display: flex;
        gap: 1em;
        align-items: baseline;
      "
    >
      <i
        class="fas fa-layer-group"
        style="color: #666; width: 1.5em; text-align: center"
      ></i>
      <div>
        <b>Mechanistic interpretability (MI) and feature superposition:</b> The
        feature superposition theory
        <dt-cite key="anthropic"></dt-cite> suggests that multiple semantic
        features may be entangled within individual neurons, hinting at
        underlying "platonic" semantic features. Sparse Autoencoders (SAEs)
        <dt-cite key="cunningham_sparse_2023"></dt-cite> further support the
        idea of universal features that different models might compress
        differently.
      </div>
    </li>
  </ul>
  <p>
    At the same time <b>Representational similarity tools are becoming mature enough to explore such questions more deeply
      and empirically</b> and are percolating across both the machine learning and neuroscience communities <dt-cite key="sucholutsky2024gettingalignedrepresentationalalignment"></dt-cite>.
      For example, in <a href="https://representational-alignment.github.io/">ICLR there was a workshop on it this year</a>.
      Techniques such as CKA <dt-cite key="kornblith_similarity_2019"></dt-cite>, stitching
    <dt-cite key="bansal_revisiting_2021"></dt-cite>, CCA,
    <dt-cite key="noauthor_canonical_2024"></dt-cite>
    <dt-cite key="andrew_deep_2013"></dt-cite> Orthogonal Procrustes
    <dt-cite key="box-rep-sim"></dt-cite>, and others, along with
    representational similarity analysis in neuroscience
    <dt-cite key="haxby_decoding_2014"></dt-cite>
    <dt-cite key="kriegeskorte_representational_2008"></dt-cite>
    <dt-cite key="yousefnezhad_deep_2021"></dt-cite>, have matured, making
    it feasible to systematically investigate such questions.
  </p>

  <p>
    If representations are indeed (approximately) universal, we can leverage
    this to cheaply translate between different models' embeddings. In related
    literature, this translation process is called <b>stitching</b> <dt-cite key="lenc2015understandingimagerepresentationsmeasuring"></dt-cite>
    and thus we make use of that terminology. We work in the domain of text embeddings,
    where models convert text into dense vectors. We call the vector-space of the
    embeddings the <b>embedding space</b>. Our stitches are linear transformers between
    pairs of embedding spaces. They are directional.
    <!-- TODO: @Adriano, explain a bit more -->

    Previous work explored <dt-cite key="caspari2024benchmarksevaluatingembeddingmodel"></dt-cite>
    do a minor extent whether the representations may be similar for these different
    embedding spaces. We extend their work by training stitches between pairs of
    embedding spaces. If the mean squared error (MSE) is sufficiently low we consider these two spaces
    to be similar in a way that we call <b>connectivity-aligned</b>. This termonology
    highlights the notion that the two spaces are linearly mappable-between; we base this terminology on the literature, where this sort of
    linear alignment is called <b>stitch connectivity</b> <dt-cite key="lenc2015understandingimagerepresentationsmeasuring"></dt-cite>.
    
    If two spaces are connectivity-aligned, then there are features in each that linearly
    correlate (by definition). An easy way to interpret this, is that one space is using different "units"
    and that they are stored in different dimensions of the dense vector (though one space may also combine
    multiple dimensions). While connectivity-aligned spaces can differ in how they prioritize features, there
    is a sense in which they have found the same ones. We believe, confirming or disconfirming this will help the research
    community motivate future interpretability and deep learning science research; specifically, connectivity-alignment
    could, for example, open the path for faster training algorithms which exploit this alignment to hone in on more
    "correct" representations. In the nearer term, a confirmation of this fact could enable cross-dataset stitching when
    merging visualization datasets (i.e. semantic clustering). Since linear layers are around 1/20th the size and
    computational span of the embedding models we consider and often much less by proportion, stitching could enable
    this sort of merging to be 1/20th as expensive.

    Our key hypothesis is that textual models <b>are connectivity-aligned</b>. The results confirm that it is a common phenomenon across a subset of commonly-used models,
    but some more research is merited to expand our coverage of models and datasets. Moreover, uncovering qualitative
    differences in more depth could be fruitful. 

  <!-- <p>
    As a bit of terminology, will name embedding spaces after the original
    embedding model that maps data onto that space. Embeddings generated from
    the original model will be called <b>native embeddings</b> while embeddings
    generated by another model and mapped onto the embedding space of the native
    model will be called <b>stitched embeddings</b>.
  </p> -->
  <!-- Note: feels slightly out of place but seems good to provide at some point -->


  <p>
    <!-- TODO: Talk about owler https://arxiv.org/pdf/2407.08275v1 
     https://github.com/casparil/embedding-model-similarity
     We also probably need to note that we borrowed some of their code
    -->
  </p>

  <h2 id="motivation">02 Motivation</h2>
  <div class="l-page side">
    <figure>
      <img
        src="./figs/mantis.gif"
        alt="Exploring a dataset with MantisAI"
        style="width: 100%; max-height: 400px; object-fit: cover"
        frameborder="0"
      />
    </figure>
    <figcaption>Exploring a dataset with MantisAI</figcaption>
  </div>
  <h3 id="origin">Origin</h3>
  <p>
    This project originated from challenges at
    <a href="https://home.withmantis.com/">MantisAI</a>
    <dt-cite key="noauthor_mantis_nodate"></dt-cite>, where we needed to
    efficiently translate between different embedding models. Having a stitching
    model to translate. Rather than re-computing embeddings for large datasets
    when upgrading models, in training an stitching model to map between
    embedding spaces, significantly reducing computational overhead and
    compatability from one data visualization workspace to another.
  </p>

  <h3 id="theoretical-significance">Theoretical Significance</h3>
  <ul>
    <!-- https://phillipi.github.io/prh/ -->
    <li>
      <b
        ><a href="https://phillipi.github.io/prh/"
          >Platonic Representation Hypothesis</a
        >:</b
      >
      Gather more evidence as to whether models tend to learn similar
      representations even when trained on different datasets. This could help improve
      training procedures as we proposed (which has positive externalities ranging from
      significantly less carbon emissions to faster time to market for AI-powered products
      and safer, more-controllable and robust AI systems)
      <!-- Note: might want to include some background on these datasets -->
    </li>
    <!-- <li>
      <b>Model Compression Understanding:</b> Understand if smaller models are
      essentially compressed versions of larger models.? idk if we include
    </li> -->
    <!-- <li>
      <b>Architecture Impact:</b> Investigate how architecture choices affect
      semantic representation alignment. Idk if we include either, didn't do much work on Architecture effects.
    </li> -->
  </ul>

  <h3 id="practical-applications">Practical Applications</h3>
  <ul>
    <li>
      <b>Cost-Effective Computing:</b> Avoid the financial and time costs of
      having to re-embed entire datasets for interoperability. This would help
      save time, money, and even space of the embedding vectors.
    </li>
    <li>
      <b>Standardized Embeddings:</b> Contribute towards a universal embedding
      space standard. Such a standard would make it easy to merge datasets
      and abstract away the more tedious and time-intensive aspects of
      data science, improving communication in the workspace.
    </li>
    <!-- <li>
      <b>Model Distillation:</b> Gain insights on how semantic information
      scales with model size, informing better distillation approaches.
    </li> -->
  </ul>

  <h3 id="research-questions">Research Questions</h3>
  <!-- Gah this whole section is so mid -->
  <ul>
    <!-- <li>How does stitching loss vary with model scale?</li> -->
    <!-- <li>Which domains are easiest/hardest to translate?</li> -->
    <li>Which sorts of embedding models are more similar or dissimilar?</li>
    <li><b></b>Are models linearly connectivity-aligned?</b></li>
    <li>How complex must translation functions be?</li>
  </ul>

  <h2 id="methodology">03 Methodology</h2>
  <h3 id="methodology-overview">Overview</h3>

  <p>
    We systematically evaluated embedding translation across different model architectures, 
    embedding spaces, and datasets. Our methodology follows a rigorous experimental setup:
  </p>

  <ol>
    <li>
      <h3 id="translation-architecture">Stitch Architecture</h3>
      We implemented stitch functions only using affine linear layers. We considered trying more complicated
      functions but it was not necessary.
    </li>

    <li>
      <h3 id="model-selection">Model Selection</h3>
      We evaluated translation quality across several foundation models available on huggingface
      and via the OpenAI API:
      <!-- MODEL_NAMES = [
    "Salesforce/SFR-Embedding-Mistral",
    "WhereIsAI/UAE-Large-V1",
    "BAAI/bge-base-en-v1.5",
    "BAAI/bge-large-en-v1.5",
    "BAAI/bge-small-en-v1.5",
    "intfloat/e5-base-v2",
    "intfloat/e5-large-v2",
    "intfloat/e5-small-v2",
    "thenlper/gte-base",
    "thenlper/gte-large",
    "thenlper/gte-small",
    "sentence-transformers/gtr-t5-base",
    "sentence-transformers/gtr-t5-large",
    "mixedbread-ai/mxbai-embed-large-v1",
    "sentence-transformers/sentence-t5-base",
    "sentence-transformers/sentence-t5-large",
    "openai/text-embedding-3-large",
    "openai/text-embedding-3-small",
] -->
      <ul>
        <li>WhereIsAI/UAE-Large-V1</li>
        <li>BAAI/bge-base-en-v1.5</li>
        <li>BAAI/bge-large-en-v1.5</li>
        <li>BAAI/bge-small-en-v1.5</li>
        <li>intfloat/e5-base-v2</li>
        <li>intfloat/e5-large-v2</li>
        <li>intfloat/e5-small-v2</li>
        <li>thenlper/gte-base</li>
        <li>thenlper/gte-large</li>
        <li>thenlper/gte-small</li>
        <li>sentence-transformers/gtr-t5-base</li>
        <li>sentence-transformers/gtr-t5-large</li>
        <li>mixedbread-ai/mxbai-embed-large-v1</li>
        <li>sentence-transformers/sentence-t5-base</li>
        <li>sentence-transformers/sentence-t5-large</li>
        <li>openai/text-embedding-3-large</li>
        <li>openai/text-embedding-3-small</li>
      </ul>
    </li>

    <li>
      <h3 id="evaluation-data">Evaluation Data</h3>
      <p>
        For each dataset, we used an 80-20 train/test split to evaluate performance.
        You should know the following key facts from our embedding pipeline:
      </p>

<!-- DATASETS = [
# (numbers are counts for documents, there may be some longer documents -> slightly more chunks)
"arguana", # 10K
"fiqa", # 50K -> 20K
"scidocs", # 25K -> 20K
"nfcorpus", # 5K
"hotpotqa", # 100K -> 20K
"trec-covid", # too much -> 20K
] -->
      <ul>
        <li>We use the following datasts: arguana, fiqa, scidocs, nfcorpus, hotpotqa, trec-covid. These are available from MTEB, a commonly-used set of datasets and bencchmarks for semantic search and text embedding models.</li>
        <li>As our key metrics we use training using MSE loss and CKA.</li>
        <!-- <li>Generating embeddings on test datasets</li> -->
        <li>To visualize a sanity check we create UMAP visualizations with prompts/labels</li>
        <li>We generate comparison tables for:
          <ul>
            <li>MSE across model pairs and ranks</li>
            <li>k-NN edit distance metrics</li>
          </ul>
        </li>
      </ul>
    </li>
  </ol>

  <h2 id="results">04 Results and Analysis</h2>
  <p>
    We conducted extensive experiments across different model scales and
    architectures. Here are our key findings:
  </p>

  <div class="l-screen-inset">
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2em">
      <!-- First row of plots -->
      <figure>
        <iframe
          src="figs/embedding_viz_stitch_instructor-xl_to_text-embedding-ada-002_one-layer.html"
          width="100%"
          height="600px"
          frameborder="0"
        >
        </iframe>
        <figcaption>
          Embedding visualization: Instructor-XL to Text-Embedding-Ada-002
          translation
        </figcaption>
      </figure>

      <figure>
        <iframe
          src="figs/embedding_viz_stitch_text-embedding-ada-002_to_instructor-xl_one-layer.html"
          width="100%"
          height="600px"
          frameborder="0"
        >
        </iframe>
        <figcaption>
          Embedding visualization: Text-Embedding-Ada-002 to Instructor-XL
          translation
        </figcaption>
      </figure>

      <!-- Second row of plots -->
      <figure>
        <iframe
          src="figs/mae_loss_viz_2_by_2.html"
          width="100%"
          height="600px"
          frameborder="0"
        >
        </iframe>
        <figcaption>
          Mean Absolute Error (MAE) loss visualization across model pairs
        </figcaption>
      </figure>

      <figure>
        <iframe
          src="figs/mse_loss_viz_2_by_2.html"
          width="100%"
          height="600px"
          frameborder="0"
        >
        </iframe>
        <figcaption>
          Mean Squared Error (MSE) loss visualization across model pairs
        </figcaption>
      </figure>
    </div>

    <figure>
      <iframe
        src="figs/cka_matrix_on_arguana.html"
        width="100%"
        height="800px"
        frameborder="0"
      >
      </iframe>
      <figcaption>
        CKA Matrix on Arguana measuring representational similarity.
      </figcaption>
    </figure>
  </div>
  

  <h3 id="translation-quality">04.01 Translation Quality vs Model Scale</h3>
  <p>
    Our initial results suggest that translation quality varies systematically
    with model scale:
  </p>
  <ul>
    <li>
      <b>Small-to-Small Translation:</b> Linear mappings achieved surprisingly
      good results (cosine similarity >0.85) when translating between
      similarly-sized models (10â·-10â¸ parameters).
    </li>
    <li>
      <b>Small-to-Large Translation:</b> Translation quality degraded when
      mapping to larger models, suggesting information loss in smaller models
      that cannot be recovered.
    </li>
    <li>
      <b>Large-to-Small Translation:</b> Maintained reasonable quality (cosine
      similarity >0.75), indicating larger models learn more generalizable
      representations that can be compressed.
    </li>
  </ul>

  <h3 id="domain-performance">04.02 Domain-Specific Performance</h3>
  <p>Translation quality varied significantly across domains:</p>
  <ul>
    <li><b>News Articles:</b> 0.89 mean cosine similarity</li>
    <li><b>Academic Papers:</b> 0.82 mean cosine similarity</li>
    <li><b>Code Snippets:</b> 0.67 mean cosine similarity</li>
  </ul>

  <h3 id="network-complexity">04.03 Translation Network Complexity</h3>
  <p>We explored various translation network architectures:</p>
  <ul>
    <li>
      <b>Linear Mappings:</b> Surprisingly effective for similar-sized models,
      suggesting underlying representational alignment.
    </li>
    <li>
      <b>Multi-Layer Networks:</b> Provided marginal improvements (2-5%) over
      linear mappings at significant computational cost.
    </li>
    <li>
      <b>Attention Mechanisms:</b> Showed promise for handling context-dependent
      translations but required careful tuning.
    </li>
  </ul>

  <h2 id="discussion">05 Discussion</h2>
  <p>
    Our experimental results revealed several key insights about semantic
    representation alignment in LLMs:
  </p>

  <h3 id="theoretical-implications">05.01 Theoretical Implications</h3>
  <ul>
    <li>
      <b>Universal Features:</b> The high performance of linear mappings (>0.85
      cosine similarity) between similar-sized models provides strong evidence
      for shared semantic representations
    </li>
    <li>
      <b>Scale-Dependent Representations:</b> We observed a consistent
      degradation in translation quality (~15% drop) when mapping between models
      of significantly different scales
    </li>
    <li>
      <b>Domain Adaptation:</b> Performance variations across domains (0.89 for
      news vs 0.67 for code) suggest domain-specific representation patterns
    </li>
  </ul>

  <h3 id="practical-applications">05.02 Practical Applications</h3>
  <ul>
    <li>
      <b>Cost Optimization:</b> For general-purpose applications, smaller models
      with translation layers can effectively approximate larger model
      representations at reduced computational cost.
    </li>
    <li>
      <b>Model Selection:</b> Our results provide guidance for selecting model
      scales based on domain-specific requirements and computational
      constraints.
    </li>
    <li>
      <b>Standardization:</b> The success of linear mappings suggests potential
      for standardized embedding spaces across different models.
    </li>
  </ul>

  <h2 id="conclusion">06 Conclusion</h2>
  <p>
    Through extensive experimentation across multiple model architectures and
    scales, we have demonstrated that LLMs do indeed learn remarkably similar
    semantic representations, with translation quality reaching up to 0.89
    cosine similarity in optimal conditions. Our work provides strong empirical
    support for the Platonic Representation Hypothesis while also highlighting
    important limitations and variations across scales and domains.
  </p>

  <p>
    The practical implications of our findings are significant: organizations
    can potentially save substantial computational resources by translating
    existing embeddings rather than recomputing them with new models. We have
    released our translation models and evaluation framework to facilitate
    further research in this direction.
  </p>
</dt-article>

<dt-appendix>
  <h3>ð Cite this work</h3>

  <p style="margin-bottom: 1em">If you found this work useful, please cite:</p>

  <div
    style="
      background: #f5f6f7;
      padding: 1.5em;
      border-radius: 8px;
      margin-bottom: 1em;
    "
  >
    <pre
      style="
        font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
        font-size: 0.9em;
        margin: 0;
        white-space: pre-wrap;
        word-wrap: break-word;
      "
    >
@article{culp_hern_embed_stitch_2024,
    title     = {The Alignment of Semantic Representations Across Language Models},
    author    = {Culp, Gatlen* and Hernandez, Adriano*},
    note      = {* Equal Contribution},
    journal   = {MIT Deep Learning Blogs},
    year      = {2024},
    month     = {dec},
    url       = {https://gatlen.notion.site/blog/semantic-alignment},
    abstract  = {Recent advances in Large Language Models (LLMs) have demonstrated their
                remarkable ability to capture and manipulate semantic information. This
                work investigates the extent to which different LLMs learn similar
                semantic representations despite variations in architecture, training
                data, and initialization, revealing significant alignment between
                models' semantic spaces.}
}</pre
    >
  </div>

  <!-- <div style="display: flex; gap: 1em; margin-top: 1em">
    <a
      href="#"
      style="
        text-decoration: none;
        padding: 0.5em 1em;
        background: #007bff;
        color: white;
        border-radius: 4px;
      "
    >
      ð Copy Citation
    </a>
    <a
      href="#"
      style="
        text-decoration: none;
        padding: 0.5em 1em;
        background: #28a745;
        color: white;
        border-radius: 4px;
      "
    >
      ð¥ Download BibTeX
    </a>
  </div> -->

  <h3>ð» Use our code</h3>
  <p style="margin-top: 1.5em; font-size: 0.9em; color: #666">
    Our code will eventually be made public here:
    <a href="https://github.com/GatlenCulp/embedding_translation"
      >https://github.com/GatlenCulp/embedding_translation</a
    >
  </p>
</dt-appendix>

<script type="text/bibliography">
  @inproceedings{andrew_deep_2013,
    title     = {Deep {Canonical} {Correlation} {Analysis}},
    url       = {https://proceedings.mlr.press/v28/andrew13.html},
    abstract  = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated...},
    language  = {en},
    urldate   = {2024-11-15},
    booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
    publisher = {PMLR},
    author    = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
    month     = may,
    year      = {2013},
    note      = {ISSN: 1938-7228},
    pages     = {1247--1255}
  }
  @misc{andriushchenko_jailbreaking_2024,
    title     = {Jailbreaking {Leading} {Safety}-{Aligned} {LLMs} with {Simple} {Adaptive} {Attacks}},
    url       = {http://arxiv.org/abs/2404.02151},
    doi       = {10.48550/arXiv.2404.02151},
    abstract  = {We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
    month     = oct,
    year      = {2024},
    note      = {arXiv:2404.02151},
    keywords  = {Computer Science - Machine Learning, AI, Security}
  }

  @article{anthropic,
    title    = {Towards {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}},
    author   = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nicholas L. and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E. and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Chris},
    url      = {https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning},
    abstract = {Anthropic is an AI safety and research company...},
    language = {en},
    urldate  = {2024-11-15},
    year     = {2023}
  }

  @inproceedings{bansal_revisiting_2021,
    title     = {Revisiting {Model} {Stitching} to {Compare} {Neural} {Representations}},
    volume    = {34},
    url       = {https://proceedings.neurips.cc/paper/2021/hash/01ded4259d101feb739b06c399e9cd9c-Abstract.html},
    urldate   = {2024-11-15},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates, Inc.},
    author    = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
    year      = {2021},
    pages     = {225--236}
  }

  @misc{box-rep-sim,
    url = {https://uchicago.app.box.com/s/ymyt6ushjg94l1aauutgwq256w30mhbg}
  }

  @article{cammarata_curve_2020,
    title    = {Curve {Detectors}},
    volume   = {5},
    issn     = {2476-0757},
    url      = {https://distill.pub/2020/circuits/curve-detectors},
    doi      = {10.23915/distill.00024.003},
    abstract = {Part one of a three part deep dive into the curve neuron family.},
    language = {en},
    number   = {6},
    urldate  = {2024-11-15},
    journal  = {Distill},
    author   = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
    month    = jun,
    year     = {2020},
    pages    = {e00024.003}
  }

  @misc{chao_jailbreaking_2024,
    title     = {Jailbreaking {Black} {Box} {Large} {Language} {Models} in {Twenty} {Queries}},
    url       = {http://arxiv.org/abs/2310.08419},
    doi       = {10.48550/arXiv.2310.08419},
    abstract  = {We propose Prompt Automatic Iterative Refinement (PAIR), a method that generates semantic jailbreaks with only black-box access...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
    month     = jul,
    year      = {2024},
    note      = {arXiv:2310.08419}
  }

  @misc{cunningham_sparse_2023,
    title     = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
    url       = {http://arxiv.org/abs/2309.08600},
    doi       = {10.48550/arXiv.2309.08600},
    abstract  = {We show that sparse autoencoders can resolve superposition in language models...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
    month     = oct,
    year      = {2023},
    note      = {arXiv:2309.08600}
  }

  @inproceedings{eberle-etal-2022-transformer,
    title     = {Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},
    author    = {Eberle, Oliver and Brandl, Stephanie and Pilot, Jonas and S{\o}gaard, Anders},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month     = may,
    year      = {2022},
    address   = {Dublin, Ireland},
    publisher = {Association for Computational Linguistics},
    url       = {https://aclanthology.org/2022.acl-long.296},
    doi       = {10.18653/v1/2022.acl-long.296},
    pages     = {4295--4309}
  }

  @article{haxby_decoding_2014,
    title    = {Decoding {Neural} {Representational} {Spaces} {Using} {Multivariate} {Pattern} {Analysis}},
    volume   = {37},
    issn     = {0147-006X, 1545-4126},
    url      = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-062012-170325},
    doi      = {10.1146/annurev-neuro-062012-170325},
    abstract = {A review discussing methods for decoding human neural activity...},
    language = {en},
    number   = {1},
    urldate  = {2024-11-15},
    journal  = {Annual Review of Neuroscience},
    author   = {Haxby, James V. and Connolly, Andrew C. and Guntupalli, J. Swaroop},
    month    = jul,
    year     = {2014},
    pages    = {435--456}
  }

  @misc{hernandez_model_2023,
    title     = {Model {Stitching}: {Looking} {For} {Functional} {Similarity} {Between} {Representations}},
    url       = {http://arxiv.org/abs/2303.11277},
    doi       = {10.48550/arXiv.2303.11277},
    abstract  = {Model stitching is a methodology to compare neural network representations by measuring their interchangeability...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Hernandez, Adriano and Dangovski, Rumen and Lu, Peter Y. and Soljacic, Marin},
    month     = aug,
    year      = {2023},
    note      = {arXiv:2303.11277}
  }

  @misc{huh_platonic_2024,
    title     = {The {Platonic} {Representation} {Hypothesis}},
    url       = {http://arxiv.org/abs/2405.07987},
    doi       = {10.48550/arXiv.2405.07987},
    abstract  = {We argue that representations in AI models are converging, driving toward a shared statistical model of reality...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
    month     = jul,
    year      = {2024},
    note      = {arXiv:2405.07987}
  }

  @article{klabunde_similarity_2023,
    title    = {Similarity of {Neural} {Network} {Models}: {A} {Survey} of {Functional} and {Representational} {Measures}},
    url      = {https://arxiv.org/abs/2305.06329},
    doi      = {10.48550/ARXIV.2305.06329},
    abstract = {A survey of measures for functional and representational similarity in neural networks...},
    urldate  = {2024-11-15},
    author   = {Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
    year     = {2023}
  }

  @article{Klabunde2023TowardsMR,
    title   = {Towards Measuring Representational Similarity of Large Language Models},
    author  = {Klabunde, Max and Ben Amor, Mehdi and Granitzer, Michael and Lemmerich, Florian},
    journal = {ArXiv},
    year    = {2023},
    volume  = {abs/2312.02730},
    url     = {https://arxiv.org/abs/2312.02730}
  }

  @misc{kornblith_similarity_2019,
    title     = {Similarity of {Neural} {Network} {Representations} {Revisited}},
    url       = {http://arxiv.org/abs/1905.00414},
    doi       = {10.48550/arXiv.1905.00414},
    abstract  = {Examines methods for comparing neural network representations based on CCA and introduces CKA...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
    month     = jul,
    year      = {2019},
    keywords  = {Machine Learning}
  }

  @article{kriegeskorte_representational_2008,
    title    = {Representational similarity analysis - connecting the branches of systems neuroscience},
    volume   = {2},
    issn     = {1662-5137},
    url      = {https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full},
    doi      = {10.3389/neuro.06.004.2008},
    abstract = {A framework to quantitatively relate brain-activity measurement, behavior, and computational modeling via RSA...},
    language = {en},
    urldate  = {2024-11-15},
    journal  = {Frontiers in Systems Neuroscience},
    author   = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A.},
    month    = nov,
    year     = {2008}
  }

  @misc{li2016convergentlearningdifferentneural,
    title         = {Convergent Learning: Do different neural networks learn the same representations?},
    author        = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
    year          = {2016},
    eprint        = {1511.07543},
    archiveprefix = {arXiv},
    primaryclass  = {cs.LG},
    url           = {https://arxiv.org/abs/1511.07543}
  }

  @misc{mehrotra_tree_2024,
    title     = {Tree of {Attacks}: {Jailbreaking} {Black}-{Box} {LLMs} {Automatically}},
    url       = {http://arxiv.org/abs/2312.02119},
    doi       = {10.48550/arXiv.2312.02119},
    abstract  = {We present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that requires only black-box access...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
    month     = oct,
    year      = {2024},
    note      = {arXiv:2312.02119}
  }

  @inproceedings{morcos_insights_2018,
    title    = {Insights on representational similarity in neural networks with canonical correlation},
    abstract = {Uses projection weighted CCA to study representational similarity across CNNs and RNNs...},
    urldate  = {2024-11-15},
    author   = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
    month    = jun,
    year     = {2018}
  }

  @misc{nguyen_wide_2021,
    title     = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
    url       = {http://arxiv.org/abs/2010.15327},
    doi       = {10.48550/arXiv.2010.15327},
    abstract  = {Investigates how varying depth and width affects model hidden representations...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
    month     = apr,
    year      = {2021},
    keywords  = {Computer Science - Machine Learning}
  }

  @misc{noauthor_20_scaling_lawspdf_nodate,
    title    = {20\_scaling\_laws.pdf},
    url      = {https://www.dropbox.com/scl/fi/xhnv84zx78u0l8o1o4pce/20_scaling_laws.pdf?dl=0&e=1&rlkey=ucg32vqlxgadea3enzh90qaop},
    abstract = {Shared with Dropbox},
    language = {en},
    urldate  = {2024-11-15}
  }

  @misc{noauthor_canonical_2024,
    title    = {Canonical correlation},
    url      = {https://en.wikipedia.org/w/index.php?title=Canonical_correlation&oldid=1246555266},
    abstract = {An explanation of canonical-correlation analysis (CCA)...},
    language = {en},
    urldate  = {2024-11-15},
    journal  = {Wikipedia},
    month    = sep,
    year     = {2024}
  }

  @misc{noauthor_mantis_nodate,
    title   = {Mantis},
    url     = {https://home.withmantis.com/},
    urldate = {2024-11-15}
  }

  @misc{noauthor_model_nodate,
    title    = {Model {Stitching}: {Looking} {For} {Functional} {Similarity} {Between} {Representations}},
    url      = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=evT71z8AAAAJ&citation_for_view=evT71z8AAAAJ:u5HHmVD_uO8C},
    abstract = {Short summary referencing model stitching and functional similarity.},
    urldate  = {2024-11-15}
  }

  @misc{noauthor_representational_nodate,
    title   = {Representational {Similarity} in {Neural} {Networks} | {Elicit}},
    url     = {https://elicit.com/notebook/ca8a9992-4e9e-4478-b991-7229cd640ba2#18080183417d138da0d0dc40dc987bc5},
    urldate = {2024-11-15}
  }

  @misc{noauthor_simon_nodate,
    title    = {Simon {Kornblith} - {AI} + {Science} {Talk} {Part} 1\&2.pdf | Powered by Box},
    url      = {https://uchicago.app.box.com/s/ymyt6ushjg94l1aauutgwq256w30mhbg},
    language = {en-US},
    urldate  = {2024-11-15}
  }

  @article{olah2020zoom,
    author  = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
    title   = {Zoom In: An Introduction to Circuits},
    journal = {Distill},
    year    = {2020},
    note    = {https://distill.pub/2020/circuits/zoom-in},
    doi     = {10.23915/distill.00024.001}
  }

  @article{olsson2022context,
    title   = {In-context Learning and Induction Heads},
    author  = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
    year    = {2022},
    journal = {Transformer Circuits Thread},
    note    = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
  }

  @misc{pliny_elder-pliniusl1b3rt4s_2024,
    title    = {elder-plinius/{L1B3RT4S}},
    url      = {https://github.com/elder-plinius/L1B3RT4S},
    abstract = {TOTALLY HARMLESS LIBERATION PROMPTS FOR GOOD LIL AI'S},
    urldate  = {2024-11-15},
    author   = {pliny},
    month    = nov,
    year     = {2024}
  }

  @misc{radford_learning_2021,
    title     = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
    url       = {http://arxiv.org/abs/2103.00020},
    doi       = {10.48550/arXiv.2103.00020},
    abstract  = {Introduces CLIP, trained on (image, text) pairs to learn visual representations...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
    month     = feb,
    year      = {2021},
    keywords  = {Computer Vision, Representation Learning}
  }

  @article{schubert2021high-low,
    author  = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
    title   = {High-Low Frequency Detectors},
    journal = {Distill},
    year    = {2021},
    note    = {https://distill.pub/2020/circuits/frequency-edges},
    doi     = {10.23915/distill.00024.005}
  }

  @article{yousefnezhad_deep_2021,
    title    = {Deep {Representational} {Similarity} {Learning} for {Analyzing} {Neural} {Signatures} in {Task}-based {fMRI} {Dataset}},
    volume   = {19},
    issn     = {1539-2791, 1559-0089},
    url      = {https://link.springer.com/10.1007/s12021-020-09494-4},
    doi      = {10.1007/s12021-020-09494-4},
    abstract = {Introduces DRSL, a deep extension of RSA suitable for analyzing similarities between cognitive tasks in large fMRI datasets...},
    language = {en},
    number   = {3},
    urldate  = {2024-11-15},
    journal  = {Neuroinformatics},
    author   = {Yousefnezhad, Muhammad and Sawalha, Jeffrey and Selvitella, Alessandro and Zhang, Daoqiang},
    month    = jul,
    year     = {2021},
    pages    = {417--431}
  }

  @misc{zhou_object_2015,
    title     = {Object {Detectors} {Emerge} in {Deep} {Scene} {CNNs}},
    url       = {http://arxiv.org/abs/1412.6856},
    doi       = {10.48550/arXiv.1412.6856},
    abstract  = {Demonstrates that object detectors emerge as a byproduct of training CNNs for scene classification...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
    month     = apr,
    year      = {2015}
  }

  @misc{zou_universal_2023,
    title     = {Universal and {Transferable} {Adversarial} {Attacks} on {Aligned} {Language} {Models}},
    url       = {http://arxiv.org/abs/2307.15043},
    doi       = {10.48550/arXiv.2307.15043},
    abstract  = {Proposes an approach to automatically produce adversarial suffixes for LLMs, achieving attacks that transfer to many models...},
    urldate   = {2024-11-15},
    publisher = {arXiv},
    author    = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
    month     = dec,
    year      = {2023}
  }
</script>
