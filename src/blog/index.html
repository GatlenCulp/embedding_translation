<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
        background-color: #f5f9ff;
      }

      /* Hide both math displays initially, will display based on JS detection */
       .mathjax-mobile, .mathml-non-mobile { display: none; }

       /* Show the MathML content by default on non-mobile devices */
       .show-mathml .mathml-non-mobile { display: block; }
       .show-mathjax .mathjax-mobile { display: block; }

      .content-margin-container {
        display: flex;
        width: 100%; /* Ensure the container is full width */
        justify-content: left; /* Horizontally centers the children in the container */
        align-items: center;  /* Vertically centers the children in the container */
      }
      .main-content-block {
        width: 70%; /* Change this percentage as needed */
        max-width: 1100px; /* Optional: Maximum width */
        background-color: #fff;
        border-left: 1px solid #DDD;
        border-right: 1px solid #DDD;
        padding: 8px 8px 8px 8px;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      }
      .margin-left-block {
          font-size: 14px;
          width: 15%; /* Change this percentage as needed */
          max-width: 130px; /* Optional: Maximum width */
          position: relative;
          margin-left: 10px;
          text-align: left;
          font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
          padding: 5px;
      }
      .margin-right-block {
          font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
          font-size: 14px;
          width: 25%; /* Change this percentage as needed */
          max-width: 256px; /* Optional: Maximum width */
          position: relative;
          text-align: left;
          padding: 10px;  /* Optional: Adds padding inside the caption */
      }

      img {
          max-width: 100%; /* Make sure it fits inside the container */
          height: auto;
          display: block;
          margin: auto;
      }
      .my-video {
          max-width: 100%; /* Make sure it fits inside the container */
          height: auto;
          display: block;
          margin: auto;
      }
      /* Hide both video displays initially, will display based on JS detection */
       .vid-mobile, .vid-non-mobile { display: none; }

       /* Show the video content by default on non-mobile devices */
       .show-vid-mobile .vid-mobile { display: block; }
       .show-vid-non-mobile .vid-non-mobile { display: block; }

      a:link,a:visited
      {
        color: #0e7862; /*#1367a7;*/
        text-decoration: none;
      }
      a:hover {
        color: #24b597; /*#208799;*/
      }

      h1 {
        font-size: 18px;
        margin-top: 4px;
        margin-bottom: 10px;
      }

      table.header {
         font-weight: 300;
         font-size: 17px;
         flex-grow: 1;
         width: 70%;
         max-width: calc(100% - 290px);
      }
      table td, table td * {
          vertical-align: middle;
          position: relative;
      }
      table.paper-code-tab {
          flex-shrink: 0;
          margin-left: 8px;
          margin-top: 8px;
          padding: 0px 0px 0px 8px;
          width: 290px;
          height: 150px;
      }

      .layered-paper {
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35),
                5px 5px 0 0px #fff,
                5px 5px 1px 1px rgba(0,0,0,0.35),
                10px 10px 0 0px #fff,
                10px 10px 1px 1px rgba(0,0,0,0.35);
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }

      hr {
         height: 1px;
         border: none;
         background-color: #DDD;
      }

      div.hypothesis {
        width: 80%;
        background-color: #EEE;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        font-family: Courier;
        font-size: 18px;
        text-align: center;
        margin: auto;
        padding: 16px 16px 16px 16px;
      }

      div.citation {
         font-size: 0.8em;
         background-color:#fff;
         padding: 10px;
         height: 200px;
      }

      .fade-in-inline {
        position: absolute;
        text-align: center;
        margin: auto;
        -webkit-mask-image: linear-gradient(to right,
                                    transparent 0%,
                                    transparent 40%,
                                    black 50%,
                                    black 90%,
                                    transparent 100%);
        mask-image: linear-gradient(to right,
                                    transparent 0%,
                                    transparent 40%,
                                    black 50%,
                                    black 90%,
                                    transparent 100%);
        -webkit-mask-size: 8000% 100%;
        mask-size: 8000% 100%;
        animation-name: sweepMask;
        animation-duration: 4s;
        animation-iteration-count: infinite;
        animation-timing-function: linear;
        animation-delay: -1s;
      }

      .fade-in2-inline {
          animation-delay: 1s;
      }

      .inline-div {
          position: relative;
          display: inline-block;
          vertical-align: top;
          width: 50px;
      }
    </style>

    <title>The Platonic Representation Hypothesis</title>
    <meta
      property="og:title"
      content="The Platonic Representation Hypothesis"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace;
                "
                >Embedding Translation</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="https://gatlen.notion.site">Gatlen Culp</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Adriano Hernandez</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px"
                >Final project for 6.7960 Fall 2025, MIT</span
              >
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#does_x_do_y">Key Research Questions</a><br /><br />
          <a href="#implications_and_limitations"
            >Implications and limitations</a
          ><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <img src="./images/your_image_here.png" width="512px" />
      </div>
      <div class="margin-right-block">Caption for the image.</div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>
        Recent advances in Large Language Models (LLMs) highlight their ability
        to encode rich semantic representations. Yet, it remains unclear how
        consistently different models—varying in architecture, size, and
        training data—capture similar underlying semantic features. This project
        is motivated by the Platonic Representation Hypothesis (PRH), suggesting
        that universal semantic concepts might manifest across different models,
        even if they vary in scale or design. We aim to investigate the extent
        of representational similarity between different LLMs. Specifically, can
        we translate embeddings from one model’s semantic space into another’s
        with minimal loss? If this is possible, it would enable practical
        benefits such as reusing precomputed embeddings, saving on costly
        recomputations, and potentially contributing to a standardized embedding
        space shared across models. Building on preliminary evidence that neural
        networks often exhibit functionally similar components, we ask whether
        we can train a simple translation layer to map one model’s embeddings to
        another's. If successful, this could help bridge representational gaps,
        facilitate cost-effective data visualization, and refine our
        understanding of universal semantic structures in language models.
      </div>
      <div class="margin-right-block">
        Margin note: This investigation is grounded in existing literature on
        representational similarity and recent interpretability frameworks.
      </div>
    </div>

    <div class="content-margin-container" id="does_x_do_y">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Key Research Questions</h1>
        Our core questions are:
        <ul>
          <li>
            <b>Translation feasibility:</b> Can we train a mapping function
            that, given an embedding from a source model, produces an embedding
            close to that of a target model on the same input text?
          </li>
          <li>
            <b>Model scale and domain effects:</b> Does the difficulty of
            translation depend on model size (e.g. BERT-base vs. GPT-4-scale) or
            on specific data domains (news, code, social media)?
          </li>
          <li>
            <b>Representational subtlety:</b> Do smaller models fail to capture
            certain nuanced semantic features that larger models represent, and
            does this limit the fidelity of translation?
          </li>
          <li>
            <b>Complexity of translation:</b> How simple can the translation
            function be (e.g. linear mapping) while preserving semantic
            relationships?
          </li>
        </ul>

        We will use representational similarity metrics (e.g., CKA, CCA,
        Procrustes) and a variety of embeddings from different model families
        (GPT, T5, BERT, and others) to gain a systematic understanding. By
        examining different architectural choices, we hope to clarify how
        universal these semantic structures truly are.
      </div>
      <div class="margin-right-block" style="transform: translate(0%, -100%)">
        Margin note: Plato’s allegory of universal forms relates metaphorically
        here—if universal semantic features exist, can we decode them
        consistently across varied model “caves”?
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Methodology at a Glance</h1>
        We plan to:
        <ol>
          <li>Collect embeddings from multiple models on a shared dataset.</li>
          <li>
            Train a translation layer from one model’s embedding space to
            another’s and evaluate the quality of this mapping using MSE loss,
            cosine similarity, and nearest-neighbor retrieval tests.
          </li>
          <li>
            Apply representational similarity techniques to identify where
            models align or diverge in their semantic organization.
          </li>
          <li>
            Explore how translation quality scales with model size and data
            domain diversity.
          </li>
        </ol>
        This approach combines practical translation attempts with theoretical
        representational analysis.
      </div>
      <div class="margin-right-block">
        Margin note: We will also visualize embedding spaces (e.g., via UMAP) to
        qualitatively inspect how well translation preserves semantic
        clustering.
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Another section</h1>
        Below is a sample video related to embedding visualizations
        (placeholder):

        <video class="my-video" loop autoplay muted style="width: 725px">
          <source src="./images/mtsh.mp4" type="video/mp4" />
        </video>
      </div>
      <div class="margin-right-block">
        A caption for the video: dynamic embeddings changing as we translate
        from one model’s representation to another.
      </div>
    </div>

    <div class="content-margin-container" id="implications_and_limitations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Implications and limitations</h1>
        <b>Theoretical Significance:</b> Validating universal semantic features
        would strengthen our understanding of the PRH, informing theories of how
        meaning is represented in high-dimensional spaces. This could guide
        future architecture design, encouraging models that align more closely
        with these universal features.

        <br /><br /><b>Practical Applications:</b> If embedding translations are
        accurate and cost-effective, organizations could re-use embeddings from
        cheaper legacy models while upgrading to newer models only when needed.
        Standardized embedding spaces could emerge, where datasets ship with
        interoperable embeddings, reducing repeated computation and enabling
        consistent downstream analytics.

        <br /><br /><b>Limitations:</b> Perfect translation may be elusive, and
        subtle semantic nuances might not easily transfer. Some models may
        encode certain concepts in fundamentally different ways, challenging the
        idea of universal representations. Additionally, scaling beyond moderate
        model sizes could be computationally intensive, and some niche domains
        may reveal domain-specific divergences.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References (Partial):</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave"
            >Allegory of the Cave</a
          >, Plato, c. 375 BC<br /><br />
          [2] Huh et al., “Platonic Representation Hypothesis,” 2024<br /><br />
          [3] Olsson et al., “Contextual Emergence of Induction Heads,” 2022<br /><br />
          Additional references in the source proposal.
        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results</h1>

        <!-- First pair of plots -->
        <div style="display: flex; gap: 20px; justify-content: space-between; margin-bottom: 20px;">
          <iframe
            src="./figs/mse_loss_viz_2_by_2.html"
            style="width: 48%; height: 600px; border: none; max-width: 100%; min-width: 300px;"
            scrolling="no"
          >
          </iframe>

          <iframe
            src="./figs/mae_loss_viz_2_by_2.html"
            style="width: 48%; height: 600px; border: none; max-width: 100%; min-width: 300px;"
            scrolling="no"
          >
          </iframe>
        </div>

        <!-- Second pair of plots -->
        <div style="display: flex; gap: 20px; justify-content: space-between;">
          <iframe
            src="./figs/embedding_viz_stitch_instructor-xl_to_text-embedding-ada-002_one-layer.html"
            style="width: 48%; height: 600px; border: none; max-width: 100%; min-width: 300px;"
            scrolling="no"
          >
          </iframe>

          <iframe
            src="./figs/embedding_viz_stitch_text-embedding-ada-002_to_instructor-xl_one-layer.html"
            style="width: 48%; height: 600px; border: none; max-width: 100%; min-width: 300px;"
            scrolling="no"
          >
          </iframe>
        </div>
      </div>
      <div class="margin-right-block">
        Caption: MSE loss visualization comparing embedding translations between
        different models.
      </div>
    </div>
  </body>
</html>
